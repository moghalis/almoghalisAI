{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#üìöüìå Introduction: Chatting with LLaMA 2 using Hugging Face ü§óü¶ô\n",
        "In this notebook, you'll learn how to build a chat interface with the LLaMA 2 (7B) model, specifically the NousResearch/Llama-2-7b-chat-hf and meta-llama/Llama-2-7b-chat-hf variant both hosted on Hugging Face. This model is a fine-tuned version of Meta's LLaMA 2 architecture, optimized for engaging, instruction-following conversations ‚Äî similar to ChatGPT.\n",
        "\n",
        "üöÄ Objectives:\n",
        "\n",
        "- Comparing Meta's LLaMA 2 and the chat Fine-Tuned Variant\n",
        "\n",
        "- Load and use a conversational LLM from Hugging Face\n",
        "\n",
        "- Structure chat history for multi-turn dialogue\n",
        "\n",
        "- Generate coherent and context-aware responses using transformers and pipeline\n",
        "\n",
        "- Explore the power of open-source LLMs without running them locally\n",
        "\n",
        "üë®‚Äçüíª Whether you're a developer, data scientist, or AI enthusiast, this notebook helps you quickly start chatting with one of the most capable open-access LLMs available. üß†üí¨\n",
        "\n",
        "üìπ This video tutorial on my YouTube channel walks you through each step of the process ‚Äî with explanations, code execution, and real-time results. üîçüí° Make sure to follow along, try it out, and see how easy it is to chat with LLaMA 2 using Hugging Face! ü§ñ‚ú®\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8wYWEQWZ9E2L"
      },
      "id": "8wYWEQWZ9E2L"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## üîç Comparing Meta's LLaMA 2 and NousResearch's Fine-Tuned Variant üß†üí¨\n",
        "\n",
        "When working with open LLMs on Hugging Face, it's important to understand the difference between the **base models** and their **fine-tuned chat-ready variants**. In this notebook, we use the `NousResearch/Llama-2-7b-chat-hf` model ‚Äî a fine-tuned version of Meta's foundational LLaMA 2.\n",
        "\n",
        "üõ†Ô∏è [Meta's LLaMA 2](https://huggingface.co/meta-llama/Llama-2-7b-chat)\n",
        "(`meta-llama/Llama-2-7b-chat-hf`)\n",
        "\n",
        "üõ†Ô∏è [NousResearch](https://huggingface.co/NousResearch/Llama-2-7b-chat-hf) (`NousResearch/Llama-2-7b-chat-hf`)\n",
        "\n",
        "Here's a breakdown of the differences between the two:\n",
        "\n",
        "| Feature | üß™ [Meta's LLaMA 2](https://huggingface.co/meta-llama/Llama-2-7b-chat) (`meta-llama/Llama-2-7b-chat-hf`) | üõ†Ô∏è [NousResearch](https://huggingface.co/NousResearch/Llama-2-7b-chat-hf) (`NousResearch/Llama-2-7b-chat-hf`) |\n",
        "|--------|------------------------------------------------------|-----------------------------------------------------|\n",
        "| **Source** | Meta AI | Fine-tuned by NousResearch |\n",
        "| **Type** | Pretrained, base LLM,   fine-tuned for chat/instruction |\n",
        "| **Use Case** |  Conversational AI, instruction following |\n",
        "| **Access** | Requires access token + approval üîê | Publicly available on Hugging Face üÜì |\n",
        "| **Best for** |  Anyone building chatbots or dialogue systems |\n",
        "\n",
        "### ‚úÖ Summary:\n",
        "The `NousResearch` version is essentially **Meta‚Äôs LLaMA 2 (7B)** ‚Äî but **fine-tuned**, **more accessible**, and **optimized** for **real-world chat experiences**. If you're building chat applications or testing dialogue systems, it's the faster and friendlier starting point! üöÄ"
      ],
      "metadata": {
        "id": "NcxO2Cdm97-b"
      },
      "id": "NcxO2Cdm97-b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß∞ Install Required Libraries\n"
      ],
      "metadata": {
        "id": "CWRaEKc2_xwx"
      },
      "id": "CWRaEKc2_xwx"
    },
    {
      "cell_type": "code",
      "source": [
        "#%pip install -U torch==2.0.1 \\\n",
        "#  transformers==4.33.0 \\\n",
        "#  sentencepiece==0.1.99 \\\n",
        "#  accelerate==0.22.0 # needed for low_cpu_mem_usage parameter"
      ],
      "metadata": {
        "id": "4PHsB_3a6Cxf"
      },
      "id": "4PHsB_3a6Cxf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîß Load the Chat Model and Tokenizer for `NousResearch/Llama-2-7b-chat-hf` model and tokenizer from Hugging Face"
      ],
      "metadata": {
        "id": "KCo0ALVdATa4"
      },
      "id": "KCo0ALVdATa4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b48c0687-ade4-4e7e-812a-99f0cfeca9c4",
      "metadata": {
        "tags": [],
        "id": "b48c0687-ade4-4e7e-812a-99f0cfeca9c4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import LlamaTokenizer,LlamaForCausalLM\n",
        "\n",
        "model_checkpoint = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###üß© Preparing Messages for LLaMA Chat Format\n",
        "\n",
        "This utility function transforms a list of message histories into properly formatted input prompts for LLaMA-style chat models, following the instruction formatting used in many fine-tuned Hugging Face models.\n",
        "\n",
        "### üîç What the Code Does:\n",
        "- Defines a `Message` structure with roles (`system`, `user`, `assistant`) and content.\n",
        "- Prepares messages with system instructions using special tokens like `<<SYS>>` and `[INST]...[/INST]`.\n",
        "- Verifies correct message ordering:\n",
        "  - A `system` message (optional, must be first)\n",
        "  - Followed by alternating `user` and `assistant` messages\n",
        "  - Ending with a `user` message\n",
        "- Builds input strings by interleaving user and assistant turns, wrapped in `[INST]` tags, and adds `bos_token` and `eos_token` as required by the tokenizer.\n",
        "- Ensures the format is compatible with models expecting instruction-style inputs (like LLaMA-2 chat variants).\n",
        "\n",
        "üõ†Ô∏è This function is adapted from [llama-cpp-chat-completion-wrapper](https://github.com/viniciusarruda/llama-cpp-chat-completion-wrapper/blob/1c9e29b70b1aaa7133d3c7d7b59a92d840e92e6d/llama_cpp_chat_completion_wrapper.py)\n",
        "\n"
      ],
      "metadata": {
        "id": "w7CI81KDAt6i"
      },
      "id": "w7CI81KDAt6i"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eeaafdb-8482-4491-bba2-5b3b389813c3",
      "metadata": {
        "tags": [],
        "id": "4eeaafdb-8482-4491-bba2-5b3b389813c3"
      },
      "outputs": [],
      "source": [
        "# based on https://github.com/viniciusarruda/llama-cpp-chat-completion-wrapper/blob/1c9e29b70b1aaa7133d3c7d7b59a92d840e92e6d/llama_cpp_chat_completion_wrapper.py\n",
        "\n",
        "from typing import List\n",
        "from typing import Literal\n",
        "from typing import TypedDict\n",
        "\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "Role = Literal[\"system\", \"user\", \"assistant\"]\n",
        "\n",
        "class Message(TypedDict):\n",
        "    role: Role\n",
        "    content: str\n",
        "\n",
        "MessageList = List[Message]\n",
        "\n",
        "BEGIN_INST, END_INST = \"[INST] \", \" [/INST] \"\n",
        "BEGIN_SYS, END_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "\n",
        "def convert_list_of_message_lists_to_input_prompt(list_of_message_lists: List[MessageList], tokenizer: PreTrainedTokenizer) -> List[str]:\n",
        "    input_prompts: List[str] = []\n",
        "    print(type(list_of_message_lists))\n",
        "    print(type(list_of_message_lists[0]))\n",
        "    for message_list in list_of_message_lists:\n",
        "        if message_list[0][\"role\"] == \"system\":\n",
        "            content = \"\".join([BEGIN_SYS, message_list[0][\"content\"], END_SYS, message_list[1][\"content\"]])\n",
        "            message_list = [{\"role\": message_list[1][\"role\"], \"content\": content}] + message_list[2:]\n",
        "\n",
        "        if not (\n",
        "            all([msg[\"role\"] == \"user\" for msg in message_list[::2]])\n",
        "            and all([msg[\"role\"] == \"assistant\" for msg in message_list[1::2]])\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                \"Format must be in this order: 'system', 'user', 'assistant' roles.\\nAfter that, you can alternate between user and assistant multiple times\"\n",
        "            )\n",
        "\n",
        "        eos = tokenizer.eos_token\n",
        "        bos = tokenizer.bos_token\n",
        "        input_prompt = \"\".join(\n",
        "            [\n",
        "                \"\".join([bos, BEGIN_INST, (prompt[\"content\"]).strip(), END_INST, (answer[\"content\"]).strip(), eos])\n",
        "                for prompt, answer in zip(message_list[::2], message_list[1::2])\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        if message_list[-1][\"role\"] != \"user\":\n",
        "            raise ValueError(f\"Last message must be from user role. Instead, you sent from {message_list[-1]['role']} role\")\n",
        "\n",
        "        input_prompt += \"\".join([bos, BEGIN_INST, (message_list[-1][\"content\"]).strip(), END_INST])\n",
        "\n",
        "        input_prompts.append(input_prompt)\n",
        "\n",
        "    return input_prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " üß™ Creating and Formatting a Simple Chat Prompt\n",
        "\n",
        "Here we construct a basic chat scenario using the `Message` format defined earlier. Note that:\n",
        "\n",
        "- üõ† A **system message** instructing the assistant to respond only with emojis\n",
        "- üë§ A **user message** asking a questionon behalf of user\n",
        "- üß± These messages are added to a list and passed into our `convert_list_of_message_lists_to_input_prompt()` function to generate a LLaMA-compatible chat prompt\n",
        "\n",
        "This shows how to structure a minimal, valid input for models expecting `[INST]`-formatted chat inputs.\n",
        "\n",
        "üßµ The resulting `prompt` can then be passed into the model for response generation."
      ],
      "metadata": {
        "id": "atOB045ABuUS"
      },
      "id": "atOB045ABuUS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cf0ab54-bbad-42b9-a2ba-17cf4e28fbee",
      "metadata": {
        "tags": [],
        "id": "1cf0ab54-bbad-42b9-a2ba-17cf4e28fbee"
      },
      "outputs": [],
      "source": [
        "system_message = Message()\n",
        "system_message[\"role\"] = \"system\"\n",
        "system_message[\"content\"] = \"Answer only with emojis\"\n",
        "print(system_message)\n",
        "\n",
        "user_message = Message()\n",
        "user_message[\"role\"] = \"user\"\n",
        "user_message[\"content\"] = \"Who won the 1993 Stanley Hockey Cup?\"\n",
        "print(user_message)\n",
        "\n",
        "# assistant_message = Message()\n",
        "# assistant_message.role = \"assistant\"\n",
        "# assistant_message.content = \"\"\n",
        "\n",
        "list_of_messages = list()\n",
        "list_of_messages.append(system_message)\n",
        "list_of_messages.append(user_message)\n",
        "\n",
        "list_of_message_lists = list()\n",
        "list_of_message_lists.append(list_of_messages)\n",
        "\n",
        "prompt = convert_list_of_message_lists_to_input_prompt(list_of_message_lists, tokenizer)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† Generate Text from a prompt using pipeline API\n",
        "\n",
        "- Tokenize the input prompt.\n",
        "\n",
        "- Configure the length of the prompt in tokens is printed for reference.\n",
        "\n",
        "- User `GenerationConfig` object is used to control generation parameters, such as the max_new_tokens.\n",
        "\n",
        "- Finally, use `pipeline` API is called to generate text based on the model and tokenizer provided."
      ],
      "metadata": {
        "id": "oYuT4mmNCfPH"
      },
      "id": "oYuT4mmNCfPH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "703f96ed-08a1-466f-8b67-919ac7e1e1a3",
      "metadata": {
        "tags": [],
        "id": "703f96ed-08a1-466f-8b67-919ac7e1e1a3"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "tokenized_prompt = tokenizer(prompt)\n",
        "\n",
        "print(f'prompt is {len(tokenized_prompt[\"input_ids\"][0])} tokens')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_prompt"
      ],
      "metadata": {
        "id": "JmXqByW9W4Va"
      },
      "id": "JmXqByW9W4Va",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80143960-fc13-47ca-a1a1-4a22446eaebf",
      "metadata": {
        "tags": [],
        "id": "80143960-fc13-47ca-a1a1-4a22446eaebf"
      },
      "outputs": [],
      "source": [
        "from transformers import GenerationConfig\n",
        "\n",
        "generation_config = GenerationConfig(max_new_tokens=2000)\n",
        "\n",
        "pipeline = pipeline(\"text-generation\",\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer,\n",
        "                    generation_config=generation_config,\n",
        "                    return_full_text=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "702a3695-8c9a-4978-8b90-5b39ab22cf8f",
      "metadata": {
        "tags": [],
        "id": "702a3695-8c9a-4978-8b90-5b39ab22cf8f"
      },
      "outputs": [],
      "source": [
        "response = pipeline(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response[0]"
      ],
      "metadata": {
        "id": "Vo9vY0YEnG6Z"
      },
      "id": "Vo9vY0YEnG6Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response[0][0]['generated_text'])"
      ],
      "metadata": {
        "id": "kbuWNnGRSWhK"
      },
      "id": "kbuWNnGRSWhK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " üß™ Let's try a simpler prompt way to simlplify what happens inside `pipeline` API\n"
      ],
      "metadata": {
        "id": "_O-tkvXQFjJu"
      },
      "id": "_O-tkvXQFjJu"
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"<<SYS>>\\nYou are a helpful assistant that provides clear, brief and informative answers.\\n<<SYS>>\"\n",
        "\n",
        "user_prompt = \"What is the capital of Canada?\"\n",
        "\n",
        "# Prompt format\n",
        "prompt = system_prompt + \"\\n\" + user_prompt\n",
        "\n",
        "print(prompt)\n",
        "\n",
        "# 1. tokenize the prompt into tokens\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# 2. Pass the tokens to the model to generate a response in tokens\n",
        "output = model.generate(input_ids, max_length=50)\n",
        "\n",
        "# 3. Decode the response back\n",
        "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the response\n",
        "response"
      ],
      "metadata": {
        "id": "yiHqwE0PncTX"
      },
      "id": "yiHqwE0PncTX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_ids)"
      ],
      "metadata": {
        "id": "sz1n03RyXyrS"
      },
      "id": "sz1n03RyXyrS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output[0])"
      ],
      "metadata": {
        "id": "s9Dw0LNAYBBG"
      },
      "id": "s9Dw0LNAYBBG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " üß™ Calling `pipeline` API from above while passing the prompt to generate a response in a single line...\n"
      ],
      "metadata": {
        "id": "aTLHn1reGT8h"
      },
      "id": "aTLHn1reGT8h"
    },
    {
      "cell_type": "code",
      "source": [
        "response = pipeline(prompt)"
      ],
      "metadata": {
        "id": "2cJx6N5_pzlc"
      },
      "id": "2cJx6N5_pzlc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response[0]"
      ],
      "metadata": {
        "id": "rAvSGgjop0nk"
      },
      "id": "rAvSGgjop0nk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîÅ Repeating the Code with `meta-llama/Llama-2-7b-chat-hf`\n",
        "\n",
        "In this section, we're reusing the earlier code, but this time loading the **LLaMA 2 model** from **Meta AI** instead of a community model like the one from `NousResearch`.\n",
        "\n",
        "‚ö†Ô∏è **Note**: Access to Meta's LLaMA models requires logging into the Hugging Face Hub and requesting access to the model page.\n",
        "\n",
        "Make sure you have:\n",
        "- üîê Logged into Hugging Face using `notebook_login()`\n",
        "- ‚úÖ Been granted access to `meta-llama/Llama-2-7b-chat-hf`\n",
        "\n",
        "This version loads the model and tokenizer from Meta, formats a prompt using a system message and user question, tokenizes it, and generates a concise response using the LLaMA 2 model.\n",
        "\n",
        "üìå The setup uses `torch.float16` for efficient inference and `device_map=\"auto\"` to automatically assign model parts to available devices."
      ],
      "metadata": {
        "id": "WcJoZQBjGvLX"
      },
      "id": "WcJoZQBjGvLX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.üê±‚Äçüíª Hugging Face Hub Login\n",
        "\n",
        "The notebook_login() function will prompt for your credentials üîë, giving you access to the Hub's resources.\n",
        "#### How to Generate Tokens from Your Hugging Face Account\n",
        "\n",
        "1. üñ•Ô∏è **Go to Hugging Face Website**\n",
        "   - Visit [Hugging Face](https://huggingface.co/).\n",
        "\n",
        "2. üîë **Log In to Your Account**\n",
        "\n",
        "3. üë§ **Navigate to Your Settings by Clicking on your profile icon and select Settings**\n",
        "\n",
        "4. üîê **Generate a New Token** (Access Token)\n",
        "   - In the **Access Tokens** section on the left side of the settings page, click on **New Token**.\n",
        "   - Give your token a name (e.g., \"Jupyter Notebook\") and select the scope (permissions) for the token (e.g., **read**, **write**, or **admin**).\n",
        "   - Click **Generate**.\n",
        "\n",
        "5. üìÑ **Copy Your Token**\n",
        "\n",
        "6. üîÑ **Use the Token in Your Code**\n",
        "   - You can now use this token in your code, like in `notebook_login()` or when interacting with the Hugging Face Hub via the `transformers` library."
      ],
      "metadata": {
        "id": "K-2NUOXpGzg7"
      },
      "id": "K-2NUOXpGzg7"
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "FFjJjPD2uAMi"
      },
      "id": "FFjJjPD2uAMi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. üîÅ Using `meta-llama/Llama-2-7b-chat-hf` Instead of NousResearch\n",
        "\n",
        "We're repeating the same code, but this time using **Meta's LLaMA 2** model from Hugging Face: `meta-llama/Llama-2-7b-chat-hf`.\n",
        "\n",
        "- Loads the Meta model\n",
        "- Formats a prompt\n",
        "- Tokenizes it\n",
        "- Generates a concise response.\n",
        "\n",
        "**Note** : We use `torch.float16` for efficiency and `device_map=\"auto\"` for device placement.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cPxgSGlAILZd"
      },
      "id": "cPxgSGlAILZd"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
        "\n",
        "# 1. Load the model and tokenizer\n",
        "model_dir = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_dir)\n",
        "model = LlamaForCausalLM.from_pretrained(model_dir, device_map=\"auto\", torch_dtype=torch.float16)\n",
        "\n",
        "# 2. Create the system prompt\n",
        "system_prompt = \"<<SYS>>\\nYou are a helpful assistant that provides clear, brief and informative answers.\\n<<SYS>>\"\n",
        "\n",
        "# 3. Define the user prompt\n",
        "user_prompt = \"What is the capital of Canada?\"\n",
        "\n",
        "# 4. Format the whole prompt\n",
        "prompt = system_prompt + \"\\n\" + user_prompt\n",
        "\n",
        "# 5. Tokenize the prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# 6. Generate a response (Tokens)\n",
        "output = model.generate(input_ids, max_length=50)\n",
        "\n",
        "# 7. Decode the response (from generated tokens)\n",
        "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "# 8. Print the response\n",
        "print(response)"
      ],
      "metadata": {
        "id": "tIxK99ezaED2"
      },
      "id": "tIxK99ezaED2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_TVges7ncFGY"
      },
      "id": "_TVges7ncFGY",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "availableInstances": [
      {
        "_defaultOrder": 0,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.t3.medium",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 1,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.t3.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 2,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.t3.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 3,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.t3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 4,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 5,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 6,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 7,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 8,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 9,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 10,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 11,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 12,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5d.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 13,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5d.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 14,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5d.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 15,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5d.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 16,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5d.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 17,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5d.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 18,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5d.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 19,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 20,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": true,
        "memoryGiB": 0,
        "name": "ml.geospatial.interactive",
        "supportedImageNames": [
          "sagemaker-geospatial-v1-0"
        ],
        "vcpuNum": 0
      },
      {
        "_defaultOrder": 21,
        "_isFastLaunch": true,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.c5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 22,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.c5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 23,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.c5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 24,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.c5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 25,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 72,
        "name": "ml.c5.9xlarge",
        "vcpuNum": 36
      },
      {
        "_defaultOrder": 26,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 96,
        "name": "ml.c5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 27,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 144,
        "name": "ml.c5.18xlarge",
        "vcpuNum": 72
      },
      {
        "_defaultOrder": 28,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.c5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 29,
        "_isFastLaunch": true,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g4dn.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 30,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g4dn.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 31,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g4dn.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 32,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g4dn.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 33,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g4dn.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 34,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g4dn.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 35,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 61,
        "name": "ml.p3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 36,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 244,
        "name": "ml.p3.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 37,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 488,
        "name": "ml.p3.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 38,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.p3dn.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 39,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.r5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 40,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.r5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 41,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.r5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 42,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.r5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 43,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.r5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 44,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.r5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 45,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.r5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 46,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.r5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 47,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 48,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 49,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 50,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 51,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 52,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 53,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.g5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 54,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.g5.48xlarge",
        "vcpuNum": 192
      },
      {
        "_defaultOrder": 55,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 56,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4de.24xlarge",
        "vcpuNum": 96
      }
    ],
    "instance_type": "ml.g5.4xlarge",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}