{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc1TYk3PJsw2"
      },
      "source": [
        "# ü§ó Hugging Face Transformers: Master the pipeline() API üöÄ  \n",
        "## Build NLP & Vision Applications in Minutes\n",
        "\n",
        "Welcome to this hands-on tutorial on the ü§ó Hugging Face **Transformers** library ‚Äî one of the most powerful and accessible toolkits for Natural Language Processing (NLP), Computer Vision, and Speech applications.\n",
        "\n",
        "In this notebook, we‚Äôll explore how to leverage the `pipeline()` API to quickly solve a wide range of tasks such as:\n",
        "- Sentiment Analysis ‚ù§Ô∏èüò°\n",
        "- Zero-Shot Text Classification üîÆ\n",
        "- Text Generation ‚úçÔ∏èüß†\n",
        "- Fill-Mask Predictions üß©\n",
        "- Named Entity Recognition (NER) üè∑Ô∏è\n",
        "- Question Answering ‚ùì‚úÖ\n",
        "- Summarization üì∞‚úÇÔ∏è\n",
        "- Image Classification üì∑üß†\n",
        "- Automatic Speech Recognition (ASR) üéôÔ∏èüó£Ô∏è\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ What You'll Learn\n",
        "\n",
        "By the end of this tutorial, you'll be able to:\n",
        "‚úÖ Understand what Hugging Face `pipelines` are and how they simplify ML workflows  \n",
        "‚úÖ Use pre-trained models for NLP, Vision, and ASR tasks with just a few lines of code  \n",
        "‚úÖ Interpret model outputs and understand the underlying logic  \n",
        "‚úÖ Explore deeper components like tokenizers, attention masks, and AutoModel classes\n",
        "\n",
        "\n",
        "`Hugging Face tutorial`, `Transformers pipeline`, `pipeline() API`, `NLP with Transformers`, `Hugging Face beginner guide`, `text classification`, `zero-shot learning`, `ASR`, `image classification transformers`, `Hugging Face notebook tutorial`, `fine-tune Hugging Face models`\n",
        "\n",
        "Let‚Äôs get started! üëá\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öôÔ∏è Step 1: Environment Setup\n",
        "\n",
        "Before we start using Hugging Face Transformers, make sure the necessary libraries are installed:\n",
        "\n",
        "- `transformers`: The main Hugging Face library\n",
        "- `datasets`: For loading benchmark datasets (optional for some pipelines)\n",
        "- `evaluate`: For metrics (optional)\n",
        "- `sentencepiece`: Needed for some multilingual models like MarianMT and mBART"
      ],
      "metadata": {
        "id": "b4Ga5zv6NpGn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Mwrd9I8Jszg"
      },
      "outputs": [],
      "source": [
        "#!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§ñ What is `pipeline()` in Hugging Face?\n",
        "\n",
        "The `pipeline()` API is a high-level interface provided by Hugging Face Transformers.\n",
        "\n",
        "It allows you to:\n",
        "‚úÖ Quickly use powerful pre-trained models  \n",
        "‚úÖ Perform common ML tasks (e.g., sentiment analysis, summarization)  \n",
        "‚úÖ Skip low-level model/tokenizer loading unless needed  \n",
        "‚úÖ Handle preprocessing and postprocessing automatically\n",
        "\n",
        "Think of it as a plug-and-play tool for working with Transformers!\n",
        "\n",
        "### ‚ú® Example Pipelines You‚Äôll Explore\n",
        "- `\"sentiment-analysis\"`\n",
        "- `\"zero-shot-classification\"`\n",
        "- `\"text-generation\"`\n",
        "- `\"fill-mask\"`\n",
        "- `\"ner\"` (Named Entity Recognition)\n",
        "- `\"question-answering\"`\n",
        "- `\"summarization\"`\n",
        "- `\"translation\"`\n",
        "- `\"image-classification\"`\n",
        "- `\"automatic-speech-recognition\"`\n"
      ],
      "metadata": {
        "id": "TJbDU1-Y2ixR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† Sentiment Analysis\n",
        "\n",
        "Sentiment analysis is one of the most common NLP tasks. It involves determining whether a given piece of text expresses a **positive**, **negative**, or sometimes **neutral** opinion.\n",
        "\n",
        "The Hugging Face `pipeline(\"sentiment-analysis\")` makes it incredibly easy to get started with just a few lines of code.\n",
        "\n",
        "### ‚úÖ Use Cases:\n",
        "- Product and movie review classification  \n",
        "- Social media sentiment monitoring  \n",
        "- Customer support analytics\n",
        "\n",
        "### üîπ Example 1: Single-Line Sentiment Analysis\n",
        "\n",
        "You can quickly test the sentiment of a single sentence or short passage using the pipeline. Here's a basic example.\n"
      ],
      "metadata": {
        "id": "Bv9o3nKGYnsG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYEb15NSJszp"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the sentiment analysis pipeline\n",
        "#Default Model: distilbert/distilbert-base-uncased-finetuned-sst-2-english\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Analyze a short sentence\n",
        "sentiment_pipeline([\"I absolutely loved this product! It exceeded my expectations.\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîπ Example 2: Batch Sentiment Analysis (Multiple Sentences)\n",
        "\n",
        "The pipeline also supports **batch processing**. You can analyze several reviews or opinions in one call.\n",
        "\n",
        "üóíÔ∏è What It Does? IT Processes multiple inputs efficiently. This is incredibly useful for analyzing reviews, survey responses, or social media posts in bulk."
      ],
      "metadata": {
        "id": "crTQyEzzPmlU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfKcA15JJszv"
      },
      "outputs": [],
      "source": [
        "# Run sentiment analysis on a batch of 3 texts with varying tone\n",
        "sentiment_pipeline(\n",
        "    [\n",
        "        # 1. Clearly positive, 2. Clearly negative, 3. More neutral/mixed\n",
        "        \"I absolutely loved this product! It exceeded my expectations in every way and the customer service was fantastic. Highly recommend!\",\n",
        "        \"This was a complete disappointment. The item arrived broken, and no one responded to my support emails. I will never order from this company again.\",\n",
        "        \"It works as intended, though there‚Äôs nothing particularly special about it.\"\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Zero-Shot Classification with Hugging Face Transformers\n",
        "\n",
        "The `zero-shot-classification` pipeline allows us to classify text into custom labels *without* the need for training on those specific categories. It uses models like BART or RoBERTa fine-tuned on NLI (Natural Language Inference) to evaluate whether a given label is an appropriate description of the text. This is especially useful when predefined labels are unavailable or dynamic.\n",
        "\n",
        "üîπ **Use case:** Categorize reviews, support tickets, or social media posts into topics like *\"billing\"*, *\"technical issue\"*, or *\"general feedback\"* ‚Äî even if the model was never trained on those exact classes.\n"
      ],
      "metadata": {
        "id": "inNyaXov907Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pUYLny0Jszx"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "#Default Model:  facebook/bart-large-mnli\n",
        "text_classifier = pipeline(\"zero-shot-classification\")\n",
        "text_classifier(\n",
        "    \"I contacted support three times about my account being locked, but no one\"\n",
        "    \"got back to me. This is really frustrating.\",\n",
        "    candidate_labels=[\"billing issue\", \"technical problem\", \"customer support\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üéì Education Domain\n",
        "\n",
        "classifier(\n",
        "    \"Although the course materials are well-structured, the instructor rarely responds to student questions on the discussion board.\",\n",
        "    candidate_labels=[\"course content\", \"instructor feedback\", \"platform usability\", \"technical issue\"],\n",
        ")\n"
      ],
      "metadata": {
        "id": "WLF2syXSUM4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#üéì Education Domain\n",
        "\n",
        "text_classifier(\n",
        "    \"Although the course materials are well-structured, the instructor rarely responds to student questions on the discussion board.\",\n",
        "    candidate_labels=[\"course content\", \"instructor feedback\", \"platform usability\", \"technical issue\"],\n",
        ")"
      ],
      "metadata": {
        "id": "xNHF_V3__Dy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üí∞ Finance Domain\n",
        "\n",
        "\n",
        "classifier(\n",
        "    \"I noticed an unexpected charge on my credit card and I haven‚Äôt received any explanation from the bank yet.\",\n",
        "    candidate_labels=[\"fraud\", \"billing error\", \"customer support\", \"loan inquiry\"],\n",
        ")"
      ],
      "metadata": {
        "id": "Goy_9aI_-bkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üí∞ Finance Domain\n",
        "\n",
        "text_classifier(\n",
        "    \"I noticed an unexpected charge on my credit card and I haven‚Äôt received any explanation from the bank yet.\",\n",
        "    candidate_labels=[\"fraud\", \"billing error\", \"customer support\", \"loan inquiry\"],\n",
        ")"
      ],
      "metadata": {
        "id": "3fHMZrwYZTwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úçÔ∏è Text Generation\n",
        "\n",
        "Text generation is a core capability of generative language models family like GPT-2, GPT-Neo, and Falcon. Given a starting prompt, the model **generates a coherent continuation** based on language patterns it learned during pretraining.\n",
        "\n",
        "This is widely used for:\n",
        "- AI writing assistants ‚úçÔ∏è  \n",
        "- Story or poetry generation üìñ  \n",
        "- Code autocompletion ü§ñ  \n",
        "- Educational content or chatbot responses üí¨  \n",
        "\n"
      ],
      "metadata": {
        "id": "wtmOOgYjZWK8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZydumdjUJsz1"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the text generation pipeline\n",
        "#Default model: openai-community/gpt2\n",
        "text_generator = pipeline(\"text-generation\")\n",
        "\n",
        "#You can adjust generation using args: max_length, temperature, top_k, and more to control creativity.\n",
        "text_generator(\"This tutorial will teach you how to use the Hugging Face Transformers library to\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîÅ Try a Lighter Model: `distilgpt2`\n",
        "\n",
        "If you're working with limited hardware or want faster inference, try a distilled version of GPT-2.\n"
      ],
      "metadata": {
        "id": "NQBqiv0AZvd7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tw6sLoNoJsz7"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# This is a lighter version of GPT-2 for faster generation\n",
        "text_generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "# Generate a continuation using the smaller model and adjusting parameters\n",
        "text_generator(\"This tutorial will teach you how to use the Hugging Face Transformers library to\",\n",
        "               num_return_sequences=2,\n",
        "               max_length=50,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ö° Try a Larger Instruction-Tuned Model: `tiiuae/falcon-7b-instruct`\n",
        "\n",
        "The `Falcon` models are powerful, open-access LLMs. The `instruct` variant is designed for following human instructions ‚Äî making it great for question answering, tutoring, and natural completions.\n",
        "This will take quiet time to run on regular CPU..."
      ],
      "metadata": {
        "id": "4UVqXShOyaNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the Falcon 7B instruction-following model\n",
        "text_generator = pipeline(\"text-generation\", model=\"tiiuae/falcon-7b-instruct\")\n",
        "\n",
        "# Generate a response that simulates instruction-following\n",
        "text_generator(\"This tutorial will teach you how to use the Hugging Face Transformers library to\", max_length=50)\n",
        "#Do you have enough memory...?!\n",
        "#OutOfMemoryError: CUDA out of memory. Tried to allocate 316.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 47.38 MiB is free."
      ],
      "metadata": {
        "id": "9PpAL1NsAor0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß© Fill-Mask: Masked Language Modeling\n",
        "\n",
        "The `fill-mask` pipeline predicts missing words in a sentence by filling a masked token like `[MASK]` or `<mask>` (depending on the model) with the most probable word for each masking tag based on context. It returns multiple top predictions with associated confidence scores.\n",
        "\n",
        "It‚Äôs primarily useful for:\n",
        "- Exploring language model predictions\n",
        "- Completing partially written sentences\n",
        "- Pre-training tasks (like BERT-style MLM)\n",
        "- It can be also used for classification (Trick)\n",
        "\n",
        "Let‚Äôs see it in action with some examples.\n"
      ],
      "metadata": {
        "id": "BSQauPckaNnv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3TWdoqt8Jsz-"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the fill-mask pipeline\n",
        "# Default Model: distilbert/distilroberta-base\n",
        "fill_mask = pipeline(\"fill-mask\")\n",
        "\n",
        "fill_mask(\"Transformers are a powerful tool for solving <mask> tasks.\", top_k=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîç Use Case: Domain-Specific Language Understanding\n",
        "\n",
        "We‚Äôll now use the fill-mask pipeline to complete sentences in different domains:\n",
        "- üí∞ Finance\n",
        "- üéì Education\n",
        "- ‚öñÔ∏è Legal\n",
        "\n",
        "This helps assess how well the model understands specialized language or context.\n"
      ],
      "metadata": {
        "id": "t3cvleunaU43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üí∞ Finance Domain\n",
        "print(\"Finance Example:\")\n",
        "print(fill_mask(\"The company's quarterly <mask> exceeded market expectations.\"))\n",
        "\n",
        "# üéì Education Domain\n",
        "print(\"\\nEducation Example:\")\n",
        "print(fill_mask(\"Students are encouraged to submit their assignments before the <mask>.\"))\n",
        "\n",
        "# ‚öñÔ∏è Legal Domain\n",
        "print(\"\\nLegal Example:\")\n",
        "print(fill_mask(\"According to the new law, all businesses must <mask> with the updated regulations.\"))"
      ],
      "metadata": {
        "id": "_iIPwuYeCDme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üïµÔ∏è Named Entity Recognition (NER)\n",
        "\n",
        "Named Entity Recognition identifies key elements in text such as names of people, organizations, locations, and more. For example, we can detects and group entities like \"Yoshua Bengio\" (Person), \"Mila\" (Organization), and \"Montreal\" (Location).\n",
        "\n",
        "This can be useful for tagging and highlighting critical content.\n",
        "\n",
        "NER is used in:\n",
        "- Information extraction from documents\n",
        "- Search engine enhancement\n",
        "- News tagging and trend analysis\n",
        "- Resume or medical record parsing\n"
      ],
      "metadata": {
        "id": "QFI4kcmRbhBP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SJqDHqnJs0G"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the NER pipeline with grouped entities (e.g., full names vs individual tokens)\n",
        "#Default Model: dbmdz/bert-large-cased-finetuned-conll03-english\n",
        "ner = pipeline(\"ner\", grouped_entities=True)\n",
        "\n",
        "ner(\"Yoshua Bengio, founder of Mila, gave a keynote in Montreal last Monday to highlight the institute's latest breakthroughs in responsible AI research.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ùì Question Answering (Extractive QA)\n",
        "\n",
        "The `question-answering` pipeline extracts an answer span from a given context. The pipeline searches the context for the most relevant span that answers the question and returns the answer and confidence score\n",
        "\n",
        "It‚Äôs widely used in:\n",
        "- Chatbots\n",
        "- Document search assistants\n",
        "- Educational tools and tutoring systems\n"
      ],
      "metadata": {
        "id": "F_fyihAVcAoa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g79f9bNFJs0K"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the QA pipeline\n",
        "# Default Model: distilbert/distilbert-base-cased-distilled-squad\n",
        "qa = pipeline(\"question-answering\")\n",
        "# Provide context and ask a question\n",
        "qa(\n",
        "    question=\"Who gave a keynote in Montreal?\",\n",
        "    context=\"Yoshua Bengio, founder of Mila, gave a keynote in Montreal last \"\n",
        "    \"Monday to highlight the institute's latest breakthroughs in responsible AI \"\n",
        "    \"research.\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üì∞ Text Summarization\n",
        "\n",
        "Summarization is the task of generating a concise version of a longer text while preserving its meaning.\n",
        "\n",
        "Two common types:\n",
        "- **Extractive**: Selects key sentences from the original\n",
        "- **Abstractive**: Generates new phrasing, like how a human would summarize\n",
        "\n",
        "Great for:\n",
        "- Summarizing articles, emails, research papers\n",
        "- Creating executive summaries or news digests\n"
      ],
      "metadata": {
        "id": "y-UFLD4WcJgg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEZ_9rTZJs0M"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the summarization pipeline\n",
        "#Default Model : sshleifer/distilbart-cnn-12-6\n",
        "text_summarizer = pipeline(\"summarization\")\n",
        "text_summarizer(\n",
        "    \"\"\"\n",
        "    Mila, the Quebec Artificial Intelligence Institute, continues to play a leading\n",
        "    role in shaping the future of ethical AI. Under the guidance of renowned researcher\n",
        "    Yoshua Bengio, the institute has launched several initiatives focused on\n",
        "    transparency, fairness, and social responsibility in AI development. These efforts\n",
        "    include partnerships with universities, public institutions, and tech companies\n",
        "    to promote the responsible use of machine learning models in healthcare, education,\n",
        "    and the public sector.\n",
        "\n",
        "    Last week, Mila hosted an international symposium in Montreal, drawing experts from\n",
        "    over 30 countries to discuss regulatory frameworks and AI governance. During the\n",
        "    event, Bengio emphasized the need for collaborative research and stronger safeguards\n",
        "    to ensure that AI systems align with democratic values. The symposium concluded\n",
        "    with a declaration urging governments and industry leaders to adopt ethical standards\n",
        "    for AI that prioritize human well-being over purely commercial goals.\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üåç Translation\n",
        "\n",
        "Use Hugging Face Transformers for translating text between languages using models like MarianMT and MBART.\n",
        "\n",
        "Works well for:\n",
        "- Multilingual apps or chatbots\n",
        "- Educational content delivery\n",
        "- Quick translation without external APIs\n"
      ],
      "metadata": {
        "id": "ybAI00lV2dQE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqdzWhB_Js0Q"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load a translation pipeline from English to French\n",
        "#Try the default\n",
        "text_translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
        "\n",
        "# Translate the following french sentence into an English sentence\n",
        "text_translator(\"Mila, l‚ÄôInstitut qu√©b√©cois d‚Äôintelligence artificielle, est reconnu mondialement pour ses recherches de pointe en apprentissage profond et en intelligence artificielle responsable.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üñºÔ∏è Image Classification\n",
        "\n",
        "Image classification involves predicting the main object or scene depicted in an image. Transformers like ViT (Vision Transformer) and ConvNeXt have made this possible within Hugging Face.\n",
        "\n",
        "This is useful for:\n",
        "- Object recognition\n",
        "- Content moderation\n",
        "- Sorting images into categories\n",
        "on"
      ],
      "metadata": {
        "id": "JRzQiXoN-uFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "# Load the image from the URL for COCO dataset\n",
        "image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
        "image = Image.open(requests.get(image_url, stream=True).raw)\n",
        "\n",
        "image"
      ],
      "metadata": {
        "id": "dMkLOhJQcy25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the image classification pipeline\n",
        "#Default Model: google/vit-base-patch16-224\n",
        "img_classifier = pipeline(task=\"image-classification\")\n",
        "\n",
        "image_classes_labels = img_classifier(image_url)\n",
        "#Let's print the results\n",
        "for class_label in image_classes_labels:\n",
        "    print(f\"Label: {class_label['label']}, Score: {class_label['score']:.4f}\")"
      ],
      "metadata": {
        "id": "NwOIKLqn9BWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéß Automatic Speech Recognition (ASR)\n",
        "\n",
        "ASR is the process of converting spoken language into written text using models like Wav2Vec2 or Whisper.\n",
        "\n",
        "Perfect for:\n",
        "- Transcribing audio files or lectures\n",
        "- Creating subtitles or captions\n",
        "- Building voice interfaces\n",
        "\n",
        "\n",
        "The audio sample is sourced from the Open Speech Repository, which provides freely usable speech files in multiple languages for use in speech recognition and other applications.\n"
      ],
      "metadata": {
        "id": "GBrlxabU-wv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import requests\n",
        "from IPython.display import Audio\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "# URL of the audio sample\n",
        "#\"https://www.voiptroubleshooter.com/open_speech/american/OSR_us_000_0010_8k.wav\"\n",
        "\n",
        "# Define the audio content\n",
        "audio_file ='OSR_us_000_0010_8k.wav'\n",
        "\n",
        "# Play the audio in notebook\n",
        "Audio(audio_file)\n"
      ],
      "metadata": {
        "id": "m5LtPBKL_ShM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the ASR pipeline\n",
        "# Default: facebook/wav2vec2-base-960h\n",
        "asr = pipeline(\n",
        "    task=\"automatic-speech-recognition\",\n",
        "    model=\"openai/whisper-large-v3\"\n",
        ")\n",
        "transcript = asr(\n",
        "  audio_file,\n",
        "  #return_timestamps=True\n",
        "  chunk_length_s=30,\n",
        "  #stride_length_s=5,\n",
        ")\n",
        "print(transcript)\n",
        "\n",
        "# Display the transcription\n",
        "print(\"Transcription:\", transcript[\"text\"])"
      ],
      "metadata": {
        "id": "0C9SmsHHM43Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Under the Hood: What Happens Inside pipeline() ?\n",
        "\n",
        "Behind the scenes, `pipeline()` uses the following components:\n",
        "- `AutoTokenizer`: Converts raw text into tokens\n",
        "- `AutoModel`: Loads the pre-trained neural network\n",
        "- Post-processing: Decodes and formats the model output\n",
        "\n",
        "Before we manually walk through these steps to better understand the pipeline internals, let's dive deep in `Tokenizers`.\n"
      ],
      "metadata": {
        "id": "3vLemsQcA5TR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîç Understanding Tokenizers in Transformers\n",
        "\n",
        "Before feeding text into a Transformer model, it must be **preprocessed** into numerical form. That‚Äôs the job of the **Tokenizer**.\n",
        "\n",
        "### üßæ What a Tokenizer Does:\n",
        "1. **Splits** raw text into smaller units called *tokens*  \n",
        "   - Example: `\"Transformers are awesome\"` ‚Üí `[\"Transform\", \"##ers\", \"are\", \"awesome\"]`\n",
        "2. **Maps** tokens to numerical IDs using a vocabulary  \n",
        "   - Example: `\"Transform\"` ‚Üí `20145`, `\"##ers\"` ‚Üí `2543`\n",
        "3. **Adds special tokens** required by the model  \n",
        "   - Like `[CLS]`, `[SEP]`, or `<s>`, `</s>` depending on the architecture\n",
        "\n",
        "---\n",
        "\n",
        "### üõ†Ô∏è Output of the Tokenizer:\n",
        "The tokenizer returns a dictionary with:\n",
        "- `input_ids`: Tokenized integer IDs of the text\n",
        "- `attention_mask`: 1s for real tokens, 0s for padding\n",
        "- (Optional) `token_type_ids`: For tasks like QA with multiple segments\n",
        "\n",
        "Example:\n",
        "```python\n",
        "{'input_ids': [101, 999, 2003, 2307, 102], 'attention_mask': [1, 1, 1, 1, 1]}\n"
      ],
      "metadata": {
        "id": "isUjYpj89R_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall..."
      ],
      "metadata": {
        "id": "PkDGww4H96uK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "text_classifier = pipeline(\"sentiment-analysis\")\n",
        "text_classifier(\n",
        "     [\"I absolutely loved this product! It exceeded my expectations in every way and the customer service was fantastic. Highly recommend!\",\n",
        "     \"This was a complete disappointment. The item arrived broken, and no one responded to my support emails. I will never order from this company again.\",\n",
        "     \"It works as intended, though there‚Äôs nothing particularly special about it.\"\n",
        "     ]\n",
        ")"
      ],
      "metadata": {
        "id": "qUKzbmQA_ZdZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Let's walk through these steps to better understand the pipeline internals.\n",
        " ### üßæ Step 1: Preprocessing with a Tokenizer\n",
        "\n",
        "Transformers can‚Äôt process raw text directly ‚Äî it first needs to be converted into token IDs.\n",
        "\n",
        "We use `AutoTokenizer` to:\n",
        "- Split text into tokens (words or subwords)\n",
        "- Convert tokens to integer IDs using a vocabulary\n",
        "- Add special tokens like `[CLS]` and `[SEP]`\n"
      ],
      "metadata": {
        "id": "-8SB6GpJep7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer for a classification task\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Encode a sample text\n",
        "reviews =  [\"I absolutely loved this product! It exceeded my expectations in every way and the customer service was fantastic. Highly recommend!\",\n",
        "     \"This was a complete disappointment. The item arrived broken, and no one responded to my support emails. I will never order from this company again.\",\n",
        "     \"It works as intended, though there‚Äôs nothing particularly special about it.\"\n",
        "]\n",
        "reviews_inputs = tokenizer(reviews, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "print(reviews_inputs)"
      ],
      "metadata": {
        "id": "Pi-UxZk3BQ14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View token IDs\n",
        "print(reviews_inputs[\"input_ids\"])"
      ],
      "metadata": {
        "id": "rmZEwBTHBsf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Step 2: Load the Pretrained Model\n",
        "\n",
        "Now that we have tokenized input, we feed it into a model. Let‚Äôs load the underlying architecture using `AutoModel`.\n",
        "\n",
        "#### Why You Must Use the Right Tokenizer\n",
        "Each pretrained model was trained on a specific tokenizer ‚Äî using the wrong one leads to:\n",
        "\n",
        "- Misaligned token IDs\n",
        "\n",
        "- Poor or invalid predictions\n",
        "\n",
        "- Misinterpreted input length or structure\n",
        "\n",
        "‚úÖ Always load the tokenizer from the same checkpoint as the model\n"
      ],
      "metadata": {
        "id": "StUciFDLfFNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "# Load the base transformer model (no classification head)\n",
        "model = AutoModel.from_pretrained(checkpoint)\n",
        "\n",
        "# Run the model with encoded inputs\n",
        "model_outputs = model(**reviews_inputs)\n",
        "\n",
        "# Inspect last hidden states (output embeddings)\n",
        "print(model_outputs.last_hidden_state.shape)"
      ],
      "metadata": {
        "id": "sE6yDJfBByC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üß† Choosing the Right Model Architecture in ü§ó Transformers\n",
        "\n",
        "The base class `AutoModel` only returns **hidden states** ‚Äî it's like the \"engine\" of a Transformer.\n",
        "\n",
        "To solve specific tasks (e.g., classification, QA, generation), ü§ó Transformers provides `AutoModelFor*` variants that **add a task-specific head** ‚Äî a final layer trained to produce usable outputs for that task.\n",
        "\n",
        "Below is a breakdown of what each variant adds compared to the base `AutoModel`.\n",
        "\n",
        "---\n",
        "\n",
        "##### üîß Core Model Variants (Compared to `AutoModel`)\n",
        "\n",
        "- **`AutoModel`**\n",
        "  - üîπ Outputs: `last_hidden_state`, `pooler_output` (if available)\n",
        "  - ‚ùå No task head ‚Üí requires manual postprocessing\n",
        "  - ‚ûï Ideal for embedding extraction or when building custom heads\n",
        "\n",
        "---\n",
        "\n",
        "- **`AutoModelForCausalLM`**\n",
        "  - ‚ûï **Language Modeling Head**: Linear layer projecting to vocabulary size\n",
        "  - üîπ Used for: **Next-token prediction / text generation**\n",
        "  - üîÅ Decoder-style models (e.g., GPT, Falcon)\n",
        "  - üß† Adds: `lm_head = nn.Linear(hidden_size, vocab_size, bias=False)`\n",
        "\n",
        "---\n",
        "\n",
        "- **`AutoModelForMaskedLM`**\n",
        "  - ‚ûï **Masked LM Head**: Linear + activation to predict masked tokens\n",
        "  - üîπ Used for: **Fill-mask tasks**, masked token recovery (BERT)\n",
        "  - üß† Adds: `cls.predictions = nn.Linear(hidden_size, vocab_size)`\n",
        "\n",
        "---\n",
        "\n",
        "- **`AutoModelForMultipleChoice`**\n",
        "  - ‚ûï **Choice Scoring Head**: Classification layer over each choice\n",
        "  - üîπ Used for: **Multiple-choice QA tasks** like SWAG or RACE\n",
        "  - üß† Adds: `classifier = nn.Linear(hidden_size, 1)` applied across choices\n",
        "\n",
        "---\n",
        "\n",
        "- **`AutoModelForQuestionAnswering`**\n",
        "  - ‚ûï **QA Span Head**: Two linear layers predicting start and end positions\n",
        "  - üîπ Used for: **Extractive question answering**\n",
        "  - üß† Adds:\n",
        "    - `qa_outputs = nn.Linear(hidden_size, 2)`\n",
        "    - Output shape: `(batch_size, sequence_length, 2)` ‚Üí (start, end)\n",
        "\n",
        "---\n",
        "\n",
        "- **`AutoModelForSequenceClassification`**\n",
        "  - ‚ûï **Classification Head**: Linear + dropout for label prediction\n",
        "  - üîπ Used for: **Sentiment analysis, intent detection, etc.**\n",
        "  - üß† Adds:\n",
        "    - `dropout = nn.Dropout(p)`\n",
        "    - `classifier = nn.Linear(hidden_size, num_labels)`\n",
        "\n",
        "---\n",
        "\n",
        "- **`AutoModelForTokenClassification`**\n",
        "  - ‚ûï **Token-wise Classifier Head**: Predicts a label for each token\n",
        "  - üîπ Used for: **NER, POS tagging, chunking**\n",
        "  - üß† Adds: `classifier = nn.Linear(hidden_size, num_labels)` applied over sequence\n",
        "\n",
        "---\n",
        "\n",
        "üìå Summary: All these variants wrap `AutoModel` with a **task-specific head**, so you don‚Äôt need to build one manually.\n",
        "\n",
        "> üé• Learn more in my next tutorial being prepared for recroding."
      ],
      "metadata": {
        "id": "W1U7RrPuCWnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß© Step 3: Use a Task-Specific Model (Sequence Classification)\n",
        "\n",
        "Instead of raw embeddings, we often want task-specific predictions ‚Äî like classifying sentiment.\n",
        "\n",
        "We can use `AutoModelForSequenceClassification` to load a model with the classification head.\n"
      ],
      "metadata": {
        "id": "-kKmLxUJCjKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# Load the model for sequence classification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "model_outputs = model(**reviews_inputs)\n",
        "print(\"Outputs shape:\",model_outputs.logits.shape)\n",
        "#torch.Size([3, 2]) Three sentences two labels\n",
        "print(\"Output score:\",model_outputs.logits)\n"
      ],
      "metadata": {
        "id": "ksFOxhVdCO3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Convert raw logits to probabilities and show predictions\n",
        "predictions = torch.nn.functional.softmax(model_outputs.logits, dim=-1)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "cILUNfZBC2vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üè∑Ô∏è Mapping Predictions to Human-Readable Labels\n",
        "\n",
        "To make sense of model outputs, we need to translate index-based predictions into labels.\n"
      ],
      "metadata": {
        "id": "YinWYEPegJza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View class index ‚Üí label mapping\n",
        "model.config.id2label"
      ],
      "metadata": {
        "id": "R44YbPtdC5XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's translate the decisions score into labels..."
      ],
      "metadata": {
        "id": "MjaJzrBUggaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the predicted label index\n",
        "predicted_label_indices = torch.argmax(predictions, dim=-1)\n",
        "# Map index to label\n",
        "predictions_labels = [model.config.id2label[index.item()] for index in predicted_label_indices]\n",
        "print(predictions_labels)"
      ],
      "metadata": {
        "id": "KDuaxfa0hcr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üî° Tokenization, Tokenizer, and AutoTokenizer\n",
        "\n",
        "Tokenization is the first step in turning raw text into something a model can understand. Using `AutoTokenizer`, we can easily convert text into token IDs ‚Äî and back.\n",
        "\n",
        "Let‚Äôs see how encoding and decoding work:\n"
      ],
      "metadata": {
        "id": "Lr0HXH7jE01J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß± Creating a Transformer from Scratch\n",
        "\n",
        "While most use cases rely on **pretrained models**, it‚Äôs also possible to manually configure and initialize a transformer from scratch ‚Äî starting with random weights.\n",
        "\n",
        "We‚Äôll use the `BertConfig` class to manually define the architecture, then instantiate a `BertModel` using this configuration.\n",
        "\n",
        "> ‚ö†Ô∏è Note: This model is **untrained** ‚Äî it has randomly initialized weights and won't perform well until fine-tuned on large datasets.\n"
      ],
      "metadata": {
        "id": "zV65ZdoFDW7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually define a BERT configuration\n",
        "from transformers import BertConfig, BertModel\n",
        "\n",
        "# Create a default BERT config\n",
        "bert_config = BertConfig()\n",
        "\n",
        "# Print out the configuration parameters\n",
        "print(bert_config)\n",
        "\n",
        "# Initialize a model with random weights\n",
        "model = BertModel(bert_config)\n"
      ],
      "metadata": {
        "id": "qnUris4HDZxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üìå Why This Matters\n",
        "\n",
        "- This initializes a BERT model **without pretrained weights**\n",
        "- Useful for academic experiments or custom model design\n",
        "- Not practical for production ‚Äî requires a large dataset and compute to train\n",
        "\n",
        "üîÅ Instead, we often load a pretrained model using `from_pretrained()` to save time, cost, and carbon footprint.\n"
      ],
      "metadata": {
        "id": "BG3KH-F8nbo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or loading a Transformer model that is already trained is simple ‚Äî we can do this using the from_pretrained() method:\n"
      ],
      "metadata": {
        "id": "8dmyjwzcnsQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "# The practical alternative ‚Äî load a pretrained model\n",
        "# Load weights trained on a large corpus\n",
        "model = BertModel.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "bert_folder =\"bert_model\"\n",
        "\n",
        "model.save_pretrained(bert_folder)\n"
      ],
      "metadata": {
        "id": "bVYNcfXxDxZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls bert_model"
      ],
      "metadata": {
        "id": "EllhpR6bgRS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or use AutoModel class to replace BertModel and load checkpoint-agnostice model. So we can replace one checkpoint with another given that checkpoint is trained on similar tasks."
      ],
      "metadata": {
        "id": "h7yy-P24ETME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "model = AutoModel.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "5ZamTG2KtGct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üî° Tokenization, Tokenizer, and AutoTokenizer\n",
        "\n",
        "Tokenization is the first step in turning raw text into something a model can understand. Using `AutoTokenizer`, we can easily convert text into token IDs ‚Äî and back.\n",
        "\n",
        "Let‚Äôs see how encoding and decoding work:\n"
      ],
      "metadata": {
        "id": "lY7uC9TiRdG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load tokenizer for a specific model\n",
        "model_checkpoint = \"bert-base-cased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_checkpoint)\n",
        "tokens = tokenizer(\"It works as intended, though there‚Äôs nothing particularly special about it.\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "5H-ac_iVEUBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ü§ñ Use `AutoTokenizer` for Generic Model Loading\n",
        "\n",
        "Instead of hard-coding the tokenizer class, prefer `AutoTokenizer` ‚Äî it auto-detects the correct tokenizer class based on the checkpoint.\n"
      ],
      "metadata": {
        "id": "3aTb4DsbSB14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer generically\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "tokens = tokenizer(\"It works as intended, though there‚Äôs nothing particularly special about it.\")\n",
        "\n",
        "print(\"Tokens:\", tokens)\n"
      ],
      "metadata": {
        "id": "nHsm4L8XSJPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's examine the tokenizer...."
      ],
      "metadata": {
        "id": "ocn9jRchIWwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tokenizer to disk\n",
        "tokenizer.save_pretrained(\"my_bert_tokenizer\")"
      ],
      "metadata": {
        "id": "0Y1xI3BHSSOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preview saved tokenizer files\n",
        "!ls my_bert_tokenizer"
      ],
      "metadata": {
        "id": "sfVGlnbS6V1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text =\"It works as intended, though there‚Äôs nothing particularly special about it.\"\n",
        "text_tokens = tokenizer.tokenize(text)\n",
        "\n",
        "print(text_tokens)"
      ],
      "metadata": {
        "id": "L-Y5HiDYndRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_tokens_ids = tokenizer.convert_tokens_to_ids(text_tokens)\n",
        "\n",
        "print(text_tokens_ids)"
      ],
      "metadata": {
        "id": "pV-9CA3YSo54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decode back to text\n",
        "\n",
        "text_dectoded = tokenizer.decode(text_tokens_ids)\n",
        "text_dectoded = tokenizer.decode([1135, 1759, 1112, 3005, 117, 1463, 1175, 787, 188, 1720, 2521, 1957, 1164, 1122, 119])\n",
        "print(\"Decoded text\",text_dectoded)"
      ],
      "metadata": {
        "id": "wAYfycraSqD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üßø Special Tokens\n",
        "\n",
        "Transformer models rely on special tokens for structural understanding:\n",
        "- `[CLS]`: Start of input (for classification)\n",
        "- `[SEP]`: Separator between sentences (for QA)\n",
        "- `[PAD]`: Padding\n",
        "- `[MASK]`: Used in masked language modeling\n",
        "\n",
        "These are automatically added by `AutoTokenizer`.\n"
      ],
      "metadata": {
        "id": "TS4GyJ-hI5m6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Can you spot Special Tokens IDs\n",
        "text_input_tokens = tokenizer(text, return_tensors=\"pt\")\n",
        "print(text_input_tokens[\"input_ids\"])\n"
      ],
      "metadata": {
        "id": "6NJ--BeJSxwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View model's special tokens\n",
        "print(\"Model's special tokens:\")\n",
        "print(tokenizer.special_tokens_map)\n",
        "print(tokenizer.special_tokens_map.values())\n",
        "print(tokenizer.convert_tokens_to_ids(tokenizer.special_tokens_map.values()))"
      ],
      "metadata": {
        "id": "OHNeaZRmJM5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize a new review\n",
        "review = \"The product is fine, but delivery was delayed.\"\n",
        "review_tokens = tokenizer(review)\n",
        "print(\"Tokenized:\", review_tokens)\n"
      ],
      "metadata": {
        "id": "ydLX_SwSJOoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ Sequence Classification with Attention Mask\n",
        "\n",
        "Now let‚Äôs bring everything together and demonstrate a full classification pipeline using:\n",
        "\n",
        "- `AutoTokenizer` to tokenize text\n",
        "- `AutoModelForSequenceClassification` to load a pretrained classifier\n",
        "- `attention_mask` to handle padded sequences\n",
        "- `argmax` and `id2label` to get the predicted class\n"
      ],
      "metadata": {
        "id": "nfWWz6KMvEYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load model and tokenizer for sequence classification\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "# Sample sentence for classification\n",
        "text =\"It works as intended, though there‚Äôs nothing particularly special about it.\"\n",
        "\n",
        "# Tokenize and prepare tensors\n",
        "text_tokens = tokenizer.tokenize(text)\n",
        "text_tokens_ids = tokenizer.convert_tokens_to_ids(text_tokens)\n",
        "\n",
        "text_input_ids = torch.tensor([text_tokens_ids])\n",
        "print(\"Input IDs:\", text_input_ids)\n",
        "\n",
        "model_output = model(text_input_ids)\n",
        "print(\"Logits:\", model_output.logits)\n"
      ],
      "metadata": {
        "id": "x34L3vbqTFjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üéØ Attention Masks & Padding\n",
        "#### üéØ What is Attention Mask?\n",
        "\n",
        "\n",
        "Transformer models use **attention masks** to ignore padded tokens during computation.\n",
        "\n",
        "### Why?\n",
        "Different input lengths need to be padded for batch processing. Attention masks ensure:\n",
        "- 1 ‚Üí real token\n",
        "- 0 ‚Üí padding\n",
        "\n",
        "\n",
        "An attention mask tells the model which tokens are real (1) and which are padding (0). It ensures the model **ignores padding** when computing attention weights.\n",
        "\n",
        "Let‚Äôs inspect the attention mask in action.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vWSXiriNvnkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check tokenizer's pad token ID\n",
        "tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "zq91FYb4q5KX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample token ID list for a trimmed sentence\n",
        "reviews_ids = [\n",
        "    [101, 1045, 7078, 3866, 2023, 4031],\n",
        "    [101, 2023, 4031, 2003, 2307, 102]\n",
        "]\n"
      ],
      "metadata": {
        "id": "TnCGQ0p8JtL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trimmed reviews\n",
        "reviews_ids = [\n",
        "    [101,  1045,  7078,  3866,  2023,  102],\n",
        "    [101,  2023,  2001,  1037,   102,   tokenizer.pad_token_id],\n",
        "    [  101,  2009,  2573, tokenizer.pad_token_id, tokenizer.pad_token_id,tokenizer.pad_token_id]\n",
        "]\n",
        "\n",
        "attention_mask = [\n",
        "    [1, 1, 1, 1, 1, 1],   # All tokens valid\n",
        "    [1, 1, 1, 1, 1, 0],   # One pad\n",
        "    [1, 1, 1, 0, 0, 0]    # Three pads\n",
        "]\n",
        "\n",
        "model_outputs = model(torch.tensor(reviews_ids), attention_mask=torch.tensor(attention_mask))\n",
        "print(model_outputs.logits)"
      ],
      "metadata": {
        "id": "hPcdMgRNTY_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample input reviews with different lengths\n",
        "reviews = [\n",
        "    \"This movie was amazing!\",\n",
        "    \"Terrible acting and awful plot. I don't recommend it\"\n",
        "]\n",
        "\n",
        "# Tokenize with padding\n",
        "reviews_ids = tokenizer(reviews, padding=True, return_tensors=\"pt\")\n",
        "reviews_ids['attention_mask']\n"
      ],
      "metadata": {
        "id": "PJdz3UZhLx9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Longer sequences\n",
        "With Transformer models, there is a limit to the lengths of the sequences we can pass the models. Most models handle sequences of up to 512 or 1024 tokens, and will crash when asked to process longer sequences. There are two solutions to this problem:\n",
        "\n",
        "Use a model with a longer supported sequence length.\n",
        "Truncate your sequences."
      ],
      "metadata": {
        "id": "Ph3GfDrTT3tp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Sample input reviews with different lengths\n",
        "review = \"I absolutely loved this product!\"\n",
        "reviews =  [\"I absolutely loved this product! \",\n",
        "     \"This was a complete disappointment.I will never order from this company again.\",\n",
        "     \"It works as intended.\"\n",
        "]\n",
        "\n",
        "# Tokenize with padding\n",
        "review_tokens_ids = tokenizer(reviews)\n",
        "print(review_tokens_ids)"
      ],
      "metadata": {
        "id": "ziAu6q7pT5Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìè Padding Strategies\n",
        "\n",
        "To create batches of input with equal length, we pad shorter sequences. Transformers supports:\n",
        "- `padding=True`: Pad to longest in batch\n",
        "- `padding='max_length'`: Pad to model's max input length\n",
        "\n",
        "When batching sequences of different lengths, we need to **pad** or **truncate** them to ensure uniform shape.\n",
        "\n",
        "Hugging Face‚Äôs `tokenizer()` supports flexible padding options:\n",
        "- `\"longest\"`: Pads to the length of the longest sequence in the batch\n",
        "- `\"max_length\"`: Pads to the model‚Äôs maximum length (e.g., 512 for BERT)\n",
        "- `max_length=<value>`: Pads to a user-defined max length\n",
        "\n",
        "We can also **truncate** sequences that are too long using `truncation=True`.\n",
        "\n",
        "Let‚Äôs compare them in action using a batch of example reviews."
      ],
      "metadata": {
        "id": "TqDqAvNxwNwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£ Padding up to the length of the longest sequence in the batch\n",
        "reviews_tokens = tokenizer(reviews, padding=\"longest\")\n",
        "print(\"Padding: longest\")\n",
        "print(\"Input IDs:\",reviews_tokens['input_ids'])\n",
        "print(\"Attention Mask:\",reviews_tokens['attention_mask'])\n",
        "\n",
        "\n",
        "# 2Ô∏è‚É£ Padding up to the model's max length (e.g., 512 for BERT/DistilBERT)\n",
        "reviews_tokens = tokenizer(reviews, padding=\"max_length\")\n",
        "print(\"Padding: max_length\")\n",
        "print(\"Input IDs:\",reviews_tokens['input_ids'])\n",
        "print(\"Attention Mask:\",reviews_tokens['attention_mask'])\n",
        "\n",
        "\n",
        "# 3Ô∏è‚É£ Padding to a user-defined max length (e.g., 8 tokens)\n",
        "reviews_tokens = tokenizer(reviews, padding=\"max_length\", max_length=8)\n",
        "print(\"Padding: max_length=8\")\n",
        "print(\"Input IDs:\",reviews_tokens['input_ids'])\n",
        "print(\"Attention Mask:\",reviews_tokens['attention_mask'])\n",
        "\n",
        "\n",
        "# 4Ô∏è‚É£ Truncating sequences that exceed the model‚Äôs max length\n",
        "reviews_tokens = tokenizer(reviews, truncation=True)\n",
        "\n",
        "\n",
        "# 5Ô∏è‚É£ Truncating to a custom max length (e.g., 8 tokens)\n",
        "# Will truncate the sequences that are longer than the specified max length\n",
        "model_inputs = tokenizer(reviews, max_length=8, truncation=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "pIAzaW_7UK8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Summary: When to Use What\n",
        "\n",
        "- Use `padding=\"longest\"` for memory-efficient batching in training\n",
        "- Use `padding=\"max_length\"` when exporting models or working with fixed-length inputs\n",
        "- Use `max_length=...` and `truncation=True` to limit inputs explicitly (e.g., for mobile inference)\n",
        "\n",
        "These options ensure your inputs are shaped correctly for model inference or training.\n"
      ],
      "metadata": {
        "id": "prljxMrLN2C4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÅ Tokenizer Output Formats: PyTorch, TensorFlow, NumPy\n",
        "\n",
        "The Hugging Face `tokenizer()` can return outputs in different formats depending on the framework you're working with:\n",
        "\n",
        "- `return_tensors=\"pt\"` ‚Üí returns **PyTorch tensors**\n",
        "- `return_tensors=\"tf\"` ‚Üí returns **TensorFlow tensors**\n",
        "- `return_tensors=\"np\"` ‚Üí returns **NumPy arrays**\n",
        "\n",
        "This makes it seamless to integrate Transformers into any ML workflow.\n",
        "\n",
        "Let‚Äôs compare them using the same review inputs.\n"
      ],
      "metadata": {
        "id": "aO5mxIYwOIRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£ PyTorch tensors\n",
        "reviews_tokens = tokenizer(reviews, padding=True, return_tensors=\"pt\")\n",
        "print(\"üî∂ PyTorch Tensors\")\n",
        "print(\"Input IDs:\",reviews_tokens['input_ids'])\n",
        "print(\"Attention Mask:\",reviews_tokens['attention_mask'])\n",
        "\n",
        "# 2Ô∏è‚É£ TensorFlow tensors\n",
        "reviews_tokens = tokenizer(review, padding=True, return_tensors=\"tf\")\n",
        "print(\"üî∑ TensorFlow Tensors\")\n",
        "print(\"Input IDs:\",reviews_tokens['input_ids'])\n",
        "print(\"Attention Mask:\",reviews_tokens['attention_mask'])\n",
        "\n",
        "# 3Ô∏è‚É£ NumPy arrays\n",
        "reviews_tokens = tokenizer(reviews, padding=True, return_tensors=\"np\")\n",
        "print(\"üî∂ NumPy Arrays\")\n",
        "print(\"Input IDs:\",reviews_tokens['input_ids'])\n",
        "print(\"Attention Mask:\",reviews_tokens['attention_mask'])\n"
      ],
      "metadata": {
        "id": "16MF06FQUkLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† When to Use Each Format\n",
        "\n",
        "- Use `\"pt\"` for integration with **PyTorch** models and training loops\n",
        "- Use `\"tf\"` when working in **TensorFlow/Keras**\n",
        "- Use `\"np\"` for rapid prototyping or non-deep learning preprocessing\n",
        "\n",
        "This flexibility is one of the reasons Hugging Face Transformers is framework-agnostic üí°\n"
      ],
      "metadata": {
        "id": "pJMm9OmkObJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize a review and check its components\n",
        "\n",
        "print(\"Original review text:\",review)\n",
        "review_tokenized = tokenizer(review)\n",
        "print(\"Tokens Input IDs:\",review_tokenized[\"input_ids\"])\n",
        "print(\"Attention Mask :\",review_tokenized[\"attention_mask\"])\n",
        "print(\"=========\")\n",
        "\n",
        "review_tokens = review_tokenized.tokens()\n",
        "print(\"Review tokens:\",review_tokens)\n",
        "\n",
        "review_tokens_ids = tokenizer.convert_tokens_to_ids(review_tokens)\n",
        "print(\"Review tokens converted to token IDs:\",review_tokens_ids)\n",
        "\n",
        "review_tokens_ids_withoutspecial = tokenizer.encode(review, add_special_tokens=False)\n",
        "print(\"Review tokens converted to token IDS without special:\",review_tokens_ids_withoutspecial)\n",
        "\n",
        "print(\"=========\")\n",
        "\n",
        "print(\"Review Tokens decoded:\",tokenizer.decode(review_tokens_ids))\n",
        "print(\"Recover original review:\",tokenizer.decode(review_tokens_ids, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "EDpn7vYdyIJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Final Classification Pipeline\n",
        "\n",
        "Let‚Äôs classify a review and convert the raw model logits into a human-readable label using:\n",
        "\n",
        "- `softmax()` for probabilities\n",
        "- `argmax()` for predicted class\n",
        "- `id2label` for the actual label name\n"
      ],
      "metadata": {
        "id": "Mo4RWTyVVCLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load model and tokenizer\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "short_reviews =  [\"I absolutely loved this product! \",\n",
        "     \"This was a complete disappointment.I will never order from this company again.\",\n",
        "     \"It works as intended.\"\n",
        "]\n",
        "\n",
        "# Tokenize our short reviews\n",
        "short_reviews_tokens = tokenizer(short_reviews, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "model_outputs = model(**short_reviews_tokens)\n",
        "\n",
        "print(\"Model Logits Output:\")\n",
        "print(model_outputs.logits)\n",
        "\n",
        "# Apply softmax to get probabilities\n",
        "probs = torch.nn.functional.softmax(model_outputs.logits, dim=-1)\n",
        "\n",
        "# Get predicted label index and decode\n",
        "pred_index = torch.argmax(probs, dim=-1)#.item()\n",
        "pred_labels = [model.config.id2label[index.item()] for index in pred_index]\n",
        "\n",
        "print(\"Predicted Label:\", pred_labels)"
      ],
      "metadata": {
        "id": "QP4DCHeBVBLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ Final Summary & What's Next\n",
        "\n",
        "üéâ Congratulations! You‚Äôve just completed a comprehensive hands-on tour of the ü§ó **Hugging Face Transformers** library using the powerful and beginner-friendly `pipeline()` API.\n",
        "\n",
        "---\n",
        "\n",
        "### üîç What You‚Äôve Learned:\n",
        "\n",
        "- ‚úÖ What the `pipeline()` function is and how it abstracts away model/tokenizer complexity\n",
        "- ‚úÖ How to run real-world tasks like:\n",
        "  - `sentiment-analysis` for opinion mining  \n",
        "  - `zero-shot-classification` for dynamic tagging  \n",
        "  - `text-generation` for creative writing or assistants  \n",
        "  - `fill-mask` for understanding model prediction  \n",
        "  - `ner` and `question-answering` for knowledge extraction  \n",
        "  - `summarization`, `translation`, `image-classification`, and `speech recognition`\n",
        "- ‚úÖ How tokenizers work and why they must match the model\n",
        "- ‚úÖ What special tokens, attention masks, and padding do\n",
        "- ‚úÖ How to decode logits into predicted labels manually\n",
        "- ‚úÖ The role of `AutoModel` vs `AutoModelFor*` variants\n",
        "\n",
        "---\n",
        "\n",
        "### üì¶ Takeaways:\n",
        "\n",
        "- Hugging Face makes **state-of-the-art NLP, vision, and speech** tasks accessible in minutes\n",
        "- `pipeline()` is perfect for prototyping and learning\n",
        "- For production and research, knowing what happens under the hood gives you real power\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ What's Next?\n",
        "\n",
        "Here are some recommended next steps to continue your journey:\n",
        "\n",
        "- üìö Fine-tune your own models using the `Trainer` API\n",
        "- üß™ Explore Hugging Face Datasets and Evaluate libraries\n",
        "- üß† Dive deeper into model internals with `transformers` + `accelerate`\n",
        "- üéì Subscribe and stay tuned for **my next videa**\n",
        "\n",
        "---\n",
        "\n",
        "### üôå Support the Channel\n",
        "\n",
        "If you found this notebook helpful:\n",
        "- üëç Like & Subscribe to the [almoghalisAI YouTube channel](https://www.youtube.com/@almoghalisAI)\n",
        "- ‚≠ê Star the repo on [GitHub](https://github.com/almoghalisAI)\n",
        "- üí¨ Share your feedback and questions in the comments\n",
        "\n",
        "Thanks for learning with us! See you in the next tutorial üëã\n"
      ],
      "metadata": {
        "id": "6yPyDa_5Ow6U"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KC4qnmMa7FHV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}