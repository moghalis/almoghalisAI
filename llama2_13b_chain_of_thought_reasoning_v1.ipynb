{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f6011cb7-fd15-4379-be46-02b4ffdb3fe5",
      "metadata": {
        "id": "f6011cb7-fd15-4379-be46-02b4ffdb3fe5"
      },
      "source": [
        "\n",
        "# üß† Chain of Thought (CoT) Reasoning with LLaMA-2-13B-Chat üí¨\n",
        "\n",
        "In this tutorial, we explore how the **LLaMA-2-13B-Chat-HF** model‚Äîboth from **Meta** and **NousResearch**‚Äîhandles reasoning through **Chain of Thought (CoT)** prompting. ü¶ô‚ú®\n",
        "\n",
        "You'll learn how to apply **in-context learning** using:\n",
        "- üöÄ Zero-shot prompting\n",
        "- üéØ One-shot prompting\n",
        "- üß© Few-shots prompting\n",
        "\n",
        "We walk through several examples to observe how well the model can reason through problems step by step, especially when encouraged with **CoT-style** prompts. The goal is to understand the model's **emergent reasoning ability** and how shot-based examples impact its performance.\n",
        "\n",
        "üìå Whether you're building smarter agents or exploring LLM reasoning, this notebook will give you practical insights into **prompt engineering** and **LLM behavior under various learning conditions**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%pip install -U torch==2.0.1 \\\n",
        "#  transformers==4.33.0 \\\n",
        "#  sentencepiece==0.1.99 \\\n",
        "#  accelerate==0.22.0 # needed for low_cpu_mem_usage parameter"
      ],
      "metadata": {
        "id": "9gsoqyELhznl"
      },
      "id": "9gsoqyELhznl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b48c0687-ade4-4e7e-812a-99f0cfeca9c4",
      "metadata": {
        "tags": [],
        "id": "b48c0687-ade4-4e7e-812a-99f0cfeca9c4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import LlamaTokenizer\n",
        "from transformers import LlamaForCausalLM\n",
        "\n",
        "model_checkpoint = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "#LATER WE WILL TRY WITH BIGGER MODEL \"NousResearch/Llama-2-13b-chat-hf\"\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "model = LlamaForCausalLM.from_pretrained(model_checkpoint, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
        "\n",
        "model = model.eval() #inference mode"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üö®üö®üö® Side Section Begins üö®üö®üö®\n",
        "### Using Llama-2-13b-chat-hf from Meta Llama on Hugging Face ü§ó\n",
        "  1.üê±‚Äçüíª Hugging Face Hub Login using notebook_login() function will prompt for your credentials üîë, giving you access to the Hub's resources.\n",
        "\n",
        "\n",
        "- üñ•Ô∏è **Go to Hugging Face Website**\n",
        "\n",
        "-  üîë **Log In to Your Account**\n",
        "\n",
        "-  üë§ **Navigate to Your Settings by Clicking on your profile icon and select Settings**\n",
        "\n",
        "-  üîê **Generate a New Token** (Access Token) with scope (e.g., **read**, **write**, or **admin**).\n",
        "\n",
        "-  üìÑ **Copy Your Token**\n",
        "\n",
        "-  üîÑ **Use the Token in Your Code**\n"
      ],
      "metadata": {
        "id": "V2hrG1cu_wNZ"
      },
      "id": "V2hrG1cu_wNZ"
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "RIyVpDVl_O6W"
      },
      "id": "RIyVpDVl_O6W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "  2. üîÅ Using `meta-llama/Llama-2-13b-chat-hf` ü¶ô Instead of NousResearch\n",
        "\n",
        "- Loads the Meta model\n",
        "- Load the tokenizer\n",
        "- Continue the remaining code in the notebook\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BMovAWDdB3Ai"
      },
      "id": "BMovAWDdB3Ai"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model from Meta Llama\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_checkpoint = \"meta-llama/Llama-2-13b-chat-hf\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n"
      ],
      "metadata": {
        "id": "Qt92_Dyj_FPX"
      },
      "id": "Qt92_Dyj_FPX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üö®üö®üö® End of Side Section üö®üö®üö®\n"
      ],
      "metadata": {
        "id": "VhjhpK3UBfE7"
      },
      "id": "VhjhpK3UBfE7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "###üß© Preparing Messages for LLaMA Prompt Format\n",
        "\n",
        "This utility function transforms a list of message histories into properly formatted input prompts for LLaMA-style chat models, following the instruction formatting used in many fine-tuned Hugging Face models.\n",
        "\n",
        "### üîç What the Code Does:\n",
        "- Defines a `Message` structure with roles (`system`, `user`, `assistant`) and content.\n",
        "- Prepares messages with system instructions using special tokens like `<<SYS>>` and `[INST]...[/INST]`.\n",
        "- Verifies correct message ordering:\n",
        "  - A `system` message (optional, must be first)\n",
        "  - Followed by alternating `user` and `assistant` messages\n",
        "  - Ending with a `user` message\n",
        "- Builds input strings by interleaving user and assistant turns, wrapped in `[INST]` tags, and adds `bos_token` and `eos_token` as required by the tokenizer.\n",
        "- Ensures the format is compatible with models expecting instruction-style inputs (like LLaMA-2 chat variants).\n",
        "\n",
        "üõ†Ô∏è This function is adapted from [llama-cpp-chat-completion-wrapper](https://github.com/viniciusarruda/llama-cpp-chat-completion-wrapper/blob/1c9e29b70b1aaa7133d3c7d7b59a92d840e92e6d/llama_cpp_chat_completion_wrapper.py)\n",
        "\n"
      ],
      "metadata": {
        "id": "1EXLiGp5Bg7d"
      },
      "id": "1EXLiGp5Bg7d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eeaafdb-8482-4491-bba2-5b3b389813c3",
      "metadata": {
        "tags": [],
        "id": "4eeaafdb-8482-4491-bba2-5b3b389813c3"
      },
      "outputs": [],
      "source": [
        "# based on https://github.com/viniciusarruda/llama-cpp-chat-completion-wrapper/blob/1c9e29b70b1aaa7133d3c7d7b59a92d840e92e6d/llama_cpp_chat_completion_wrapper.py\n",
        "\n",
        "from typing import List\n",
        "from typing import Literal\n",
        "from typing import TypedDict\n",
        "\n",
        "from transformers import PreTrainedTokenizer\n",
        "\n",
        "Role = Literal[\"system\", \"user\", \"assistant\"]\n",
        "\n",
        "class Message(TypedDict):\n",
        "    role: Role\n",
        "    content: str\n",
        "\n",
        "MessageList = List[Message]\n",
        "\n",
        "BEGIN_INST, END_INST = \"[INST] \", \" [/INST] \"\n",
        "BEGIN_SYS, END_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "\n",
        "def convert_list_of_message_lists_to_input_prompt(list_of_message_lists: List[MessageList], tokenizer: PreTrainedTokenizer) -> List[str]:\n",
        "    input_prompts: List[str] = []\n",
        "    print(type(list_of_message_lists))\n",
        "    print(type(list_of_message_lists[0]))\n",
        "    for message_list in list_of_message_lists:\n",
        "        if message_list[0][\"role\"] == \"system\":\n",
        "            content = \"\".join([BEGIN_SYS, message_list[0][\"content\"], END_SYS, message_list[1][\"content\"]])\n",
        "            message_list = [{\"role\": message_list[1][\"role\"], \"content\": content}] + message_list[2:]\n",
        "\n",
        "        if not (\n",
        "            all([msg[\"role\"] == \"user\" for msg in message_list[::2]])\n",
        "            and all([msg[\"role\"] == \"assistant\" for msg in message_list[1::2]])\n",
        "        ):\n",
        "            raise ValueError(\n",
        "                \"Format must be in this order: 'system', 'user', 'assistant' roles.\\nAfter that, you can alternate between user and assistant multiple times\"\n",
        "            )\n",
        "\n",
        "        eos = tokenizer.eos_token\n",
        "        bos = tokenizer.bos_token\n",
        "        input_prompt = \"\".join(\n",
        "            [\n",
        "                \"\".join([bos, BEGIN_INST, (prompt[\"content\"]).strip(), END_INST, (answer[\"content\"]).strip(), eos])\n",
        "                for prompt, answer in zip(message_list[::2], message_list[1::2])\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        if message_list[-1][\"role\"] != \"user\":\n",
        "            raise ValueError(f\"Last message must be from user role. Instead, you sent from {message_list[-1]['role']} role\")\n",
        "\n",
        "        input_prompt += \"\".join([bos, BEGIN_INST, (message_list[-1][\"content\"]).strip(), END_INST])\n",
        "\n",
        "        input_prompts.append(input_prompt)\n",
        "\n",
        "    return input_prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Zero-Shot Chain of Thought Reasoning\n",
        "\n",
        "In zero-shot learning, the model is given a new question without any examples beforehand. It must reason through the solution purely from the current prompt.\n",
        "\n",
        "**Prompt Example:**\n",
        "\n",
        "**QUESTION :**\n",
        "A factory produces both widgets and gadgets. For every 5 widgets, it uses 3 units of material A. For every 4 gadgets, it uses 6 units of material A.\n",
        "On a particular day, the factory produces 20 widgets and 16 gadgets.\n",
        "If the factory only had 50 units of material A at the start, how many units of material A will it have left after production?\n",
        "\n",
        "**ANSWER :**  \n",
        " <br />\n",
        " <br />\n",
        "\n",
        "\n",
        "**What should we expect as an answer?**"
      ],
      "metadata": {
        "id": "JMD1x58xEX1p"
      },
      "id": "JMD1x58xEX1p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cf0ab54-bbad-42b9-a2ba-17cf4e28fbee",
      "metadata": {
        "tags": [],
        "id": "1cf0ab54-bbad-42b9-a2ba-17cf4e28fbee"
      },
      "outputs": [],
      "source": [
        "system_message = Message()\n",
        "system_message[\"role\"] = \"system\"\n",
        "system_message[\"content\"] = \"\" #Note: no role for now\n",
        "print(system_message)\n",
        "\n",
        "user_message = Message()\n",
        "user_message[\"role\"] = \"user\"\n",
        "user_message[\"content\"] = \"\"\"**QUESTION :**\n",
        "A factory produces both widgets and gadgets. For every 5 widgets, it uses 3 units of material A. For every 4 gadgets, it uses 6 units of material A.\n",
        "On a particular day, the factory produces 20 widgets and 16 gadgets.\n",
        "If the factory only had 50 units of material A at the start, how many units of material A will it have left after production?\n",
        "**ANSWER :**\n",
        "\"\"\"\n",
        "\n",
        "list_of_messages = list()\n",
        "list_of_messages.append(system_message)\n",
        "list_of_messages.append(user_message)\n",
        "\n",
        "list_of_message_lists = list()\n",
        "list_of_message_lists.append(list_of_messages)\n",
        "\n",
        "prompt = convert_list_of_message_lists_to_input_prompt(list_of_message_lists, tokenizer)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "703f96ed-08a1-466f-8b67-919ac7e1e1a3",
      "metadata": {
        "tags": [],
        "id": "703f96ed-08a1-466f-8b67-919ac7e1e1a3"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "tokenized_prompt = tokenizer(prompt)\n",
        "\n",
        "print(f'prompt is {len(tokenized_prompt[\"input_ids\"][0])} tokens')\n",
        "generation_config = GenerationConfig(max_new_tokens=2000)\n",
        "\n",
        "#do_sample=True, temperature , top_p, top_k\n",
        "pipeline = pipeline(\"text-generation\", model=model,  tokenizer=tokenizer, generation_config=generation_config)\n",
        "reponse = pipeline(prompt, return_full_text=False)\n",
        "print(reponse[0][0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Is the model's answer correct?!\n",
        "**QUESTION :**\n",
        "A factory produces both widgets and gadgets. For every 5 widgets, it uses 3 units of material A. For every 4 gadgets, it uses 6 units of material A.\n",
        "On a particular day, the factory produces 20 widgets and 16 gadgets.\n",
        "If the factory only had 50 units of material A at the start, how many units of material A will it have left after production?\n",
        "**ANSWER :**\n",
        "\n",
        "For widgets: 5 widgets ‚Üí 3 units of material A.\n",
        "So, 20 widgets ‚Üí (20 √∑ 5) √ó 3 = 4 √ó 3 = 12 units.\n",
        "\n",
        "For gadgets: 4 gadgets ‚Üí 6 units of material A.\n",
        "So, 16 gadgets ‚Üí (16 √∑ 4) √ó 6 = 4 √ó 6 = 24 units.\n",
        "\n",
        "Total used = 12 + 24 = 36 units.\n",
        "Starting with 50 units: 50 ‚àí 36 = 14 units left.\n",
        "**The answer is 14.**\n",
        "\n",
        "<br />"
      ],
      "metadata": {
        "id": "_yZmeEBl99f_"
      },
      "id": "_yZmeEBl99f_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† One-Shot Chain of Thought Reasoning\n",
        "\n",
        "In one-shot learning, the model is provided with a **`single example`** before being asked to solve a similar problem. This helps it understand the expected reasoning style.\n",
        "\n",
        "**Prompt Example:**\n",
        "\n",
        "**QUESTION :**\n",
        "A tank fills in 5 hours with 2 identical pipes working together. How long would it take only one pipe to fill the same tank?\n",
        "\n",
        "**ANSWER :**\n",
        "\n",
        "If 2 pipes fill the tank in 5 hours, then 1 pipe would take 2 times longer.\n",
        "So, 5 √ó 2 = 10 hours.\n",
        "**The answer is 10.**\n",
        "\n",
        "**QUESTION :**\n",
        "A factory produces both widgets and gadgets. For every 5 widgets, it uses 3 units of material A. For every 4 gadgets, it uses 6 units of material A.\n",
        "On a particular day, the factory produces 20 widgets and 16 gadgets.\n",
        "If the factory only had 50 units of material A at the start, how many units of material A will it have left after production?\n",
        "\n",
        "**ANSWER :**\n",
        "\n",
        "<br />\n",
        "<br />\n"
      ],
      "metadata": {
        "id": "n-GK5decFKSJ"
      },
      "id": "n-GK5decFKSJ"
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = Message()\n",
        "system_message[\"role\"] = \"system\"\n",
        "system_message[\"content\"] = \"\"\n",
        "print(system_message)\n",
        "\n",
        "user_message = Message()\n",
        "user_message[\"role\"] = \"user\"\n",
        "user_message[\"content\"] = \"\"\"\n",
        "**QUESTION :**\n",
        "A tank fills in 5 hours with 2 identical pipes working together. How long would it take only one pipe to fill the same tank?\n",
        "\n",
        "**ANSWER :**\n",
        "\n",
        "If 2 pipes fill the tank in 5 hours, then 1 pipe would take 2 times longer.\n",
        "So, 5 √ó 2 = 10 hours.\n",
        "**The answer is 10.**\n",
        "\n",
        "**QUESTION :**\n",
        "A factory produces both widgets and gadgets. For every 5 widgets, it uses 3 units of material A. For every 4 gadgets, it uses 6 units of material A.\n",
        "On a particular day, the factory produces 20 widgets and 16 gadgets.\n",
        "If the factory only had 50 units of material A at the start, how many units of material A will it have left after production?\n",
        "**ANSWER :**\n",
        "\"\"\"\n",
        "\n",
        "list_of_messages = list()\n",
        "list_of_messages.append(system_message)\n",
        "list_of_messages.append(user_message)\n",
        "\n",
        "list_of_message_lists = list()\n",
        "list_of_message_lists.append(list_of_messages)\n",
        "\n",
        "prompt = convert_list_of_message_lists_to_input_prompt(list_of_message_lists, tokenizer)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "B5JiaPpsFSMv"
      },
      "id": "B5JiaPpsFSMv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from transformers import pipeline\n",
        "#from transformers import GenerationConfig\n",
        "\n",
        "tokenized_prompt = tokenizer(prompt)\n",
        "\n",
        "print(f'prompt is {len(tokenized_prompt[\"input_ids\"][0])} tokens')\n",
        "\n",
        "#generation_config = GenerationConfig(max_new_tokens=2000)\n",
        "#pipeline = pipeline(\"text-generation\", model=model,  tokenizer=tokenizer, generation_config=generation_config,)\n",
        "\n",
        "reponse = pipeline(prompt, return_full_text=False)\n",
        "print(reponse[0][0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "bq00xOdAGDDx"
      },
      "id": "bq00xOdAGDDx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WOW....!!!!!"
      ],
      "metadata": {
        "id": "42_FGCp2CHA2"
      },
      "id": "42_FGCp2CHA2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Few-Shots Chain of Thought Reasoning\n",
        "\n",
        "In few-shot learning, the model is given multiple examples that guide it to follow a specific reasoning process before solving a new problem.\n",
        "\n",
        "**Prompt Example:**\n",
        "\n",
        "**QUESTION :**\n",
        "A tank fills in 5 hours with 2 identical pipes working together. How long would it take only one pipe to fill the same tank?\n",
        "\n",
        "**ANSWER :**\n",
        "If 2 pipes fill the tank in 5 hours, then 1 pipe would take 2 times longer.\n",
        "So, 5 √ó 2 = 10 hours.\n",
        "**The answer is 10.**\n",
        "\n",
        "**QUESTION :**\n",
        "A car travels 300 km using 25 liters of fuel. How much fuel would be needed to travel 480 km at the same rate?\n",
        "\n",
        "**ANSWER :**\n",
        "300 km requires 25 liters.\n",
        "So, 1 km requires 25 √∑ 300 = 0.0833 liters.\n",
        "Then, 480 √ó 0.0833 ‚âà 40 liters.\n",
        "**The answer is 40.**\n",
        "\n",
        "**QUESTION :**\n",
        "A factory produces both widgets and gadgets. For every 5 widgets, it uses 3 units of material A. For every 4 gadgets, it uses 6 units of material A.\n",
        "On a particular day, the factory produces 20 widgets and 16 gadgets.\n",
        "If the factory only had 50 units of material A at the start, how many units of material A will it have left after production?\n",
        "\n",
        "**ANSWER :**\n",
        "\n",
        "<br />\n",
        " <br />\n"
      ],
      "metadata": {
        "id": "4JBakWh_FTRK"
      },
      "id": "4JBakWh_FTRK"
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = Message()\n",
        "system_message[\"role\"] = \"system\"\n",
        "system_message[\"content\"] = \"\"\n",
        "print(system_message)\n",
        "\n",
        "user_message = Message()\n",
        "user_message[\"role\"] = \"user\"\n",
        "user_message[\"content\"] = \"\"\"\n",
        "**QUESTION :**\n",
        "A tank fills in 5 hours with 2 identical pipes working together. How long would it take only one pipe to fill the same tank?\n",
        "\n",
        "**ANSWER :**\n",
        "\n",
        "If 2 pipes fill the tank in 5 hours, then 1 pipe would take 2 times longer.\n",
        "So, 5 √ó 2 = 10 hours.\n",
        "**The answer is 10.**\n",
        "\n",
        "**QUESTION :**\n",
        "A car travels 300 km using 25 liters of fuel. How much fuel would be needed to travel 480 km at the same rate?\n",
        "\n",
        "**ANSWER :**\n",
        "300 km requires 25 liters.\n",
        "So, 1 km requires 25 √∑ 300 = 0.0833 liters.\n",
        "Then, 480 √ó 0.0833 ‚âà 40 liters.\n",
        "**The answer is 40.**\n",
        "\n",
        "**QUESTION :**\n",
        "A factory produces both widgets and gadgets. For every 5 widgets, it uses 3 units of material A. For every 4 gadgets, it uses 6 units of material A.\n",
        "On a particular day, the factory produces 20 widgets and 16 gadgets.\n",
        "If the factory only had 50 units of material A at the start, how many units of material A will it have left after production?\n",
        "**ANSWER :**\n",
        "\"\"\"\n",
        "\n",
        "list_of_messages = list()\n",
        "list_of_messages.append(system_message)\n",
        "list_of_messages.append(user_message)\n",
        "\n",
        "list_of_message_lists = list()\n",
        "list_of_message_lists.append(list_of_messages)\n",
        "\n",
        "prompt = convert_list_of_message_lists_to_input_prompt(list_of_message_lists, tokenizer)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "gM3sJqGZGInZ"
      },
      "id": "gM3sJqGZGInZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from transformers import pipeline\n",
        "#from transformers import GenerationConfig\n",
        "\n",
        "tokenized_prompt = tokenizer(prompt)\n",
        "\n",
        "print(f'prompt is {len(tokenized_prompt[\"input_ids\"][0])} tokens')\n",
        "#generation_config = GenerationConfig(max_new_tokens=2000)\n",
        "\n",
        "#pipeline = pipeline(\"text-generation\", model=model,  tokenizer=tokenizer, generation_config=generation_config)\n",
        "reponse = pipeline(prompt, return_full_text=False)\n",
        "print(reponse[0][0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "banEPLF5FX6N"
      },
      "id": "banEPLF5FX6N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß†üß†üß† NOW, Let's try bigger model üß†üß†üß†\n",
        "### Let's try: **NousResearch/Llama-2-13b-chat-hf**\n"
      ],
      "metadata": {
        "id": "5-UrEvTTC88w"
      },
      "id": "5-UrEvTTC88w"
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"NousResearch/Llama-2-13b-chat-hf\"\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "model = LlamaForCausalLM.from_pretrained(model_checkpoint, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n",
        "\n",
        "model = model.eval() #inference mode\n",
        "\n"
      ],
      "metadata": {
        "id": "jMDcQTcLC8fJ"
      },
      "id": "jMDcQTcLC8fJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero Shot Learning"
      ],
      "metadata": {
        "id": "eeSfpfZMEXBQ"
      },
      "id": "eeSfpfZMEXBQ"
    },
    {
      "cell_type": "code",
      "source": [
        "#Build the Prompt - Zero shot\n",
        "system_message = Message()\n",
        "system_message[\"role\"] = \"system\"\n",
        "system_message[\"content\"] = \"\"\n",
        "\n",
        "user_message = Message()\n",
        "user_message[\"role\"] = \"user\"\n",
        "user_message[\"content\"] = \"\"\"\n",
        "**QUESTION :**\n",
        "A bakery sells cupcakes in boxes of 6. If a customer buys 8 boxes, and then eats 6 cupcakes, how many cupcakes does the customer have left?\n",
        "**ANSWER :**\n",
        "\"\"\"\n",
        "\n",
        "list_of_messages = list()\n",
        "list_of_messages.append(system_message)\n",
        "list_of_messages.append(user_message)\n",
        "\n",
        "list_of_message_lists = list()\n",
        "list_of_message_lists.append(list_of_messages)\n",
        "\n",
        "prompt = convert_list_of_message_lists_to_input_prompt(list_of_message_lists, tokenizer)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "VKRtObK_EWgt"
      },
      "id": "VKRtObK_EWgt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from transformers import GenerationConfig\n",
        "\n",
        "tokenized_prompt = tokenizer(prompt)\n",
        "\n",
        "print(f'prompt is {len(tokenized_prompt[\"input_ids\"][0])} tokens')\n",
        "generation_config = GenerationConfig(max_new_tokens=2000)\n",
        "\n",
        "pipeline = pipeline(\"text-generation\", model=model,  tokenizer=tokenizer, generation_config=generation_config)\n",
        "reponse = pipeline(prompt, return_full_text=False)\n",
        "print(reponse[0][0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "hfyzudMBEM-H"
      },
      "id": "hfyzudMBEM-H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† Now it is your turn..:\n",
        "**Remember: Use CoT + exmaples to boost model output**\n",
        "\n",
        "- Try few shots (How many examples do you think your model needs in the prompt?)\n",
        "- Swap examples, add more examples\n",
        "- Does the bigger model talks more than required?\n",
        "- Do you need to specify role in the system message?\n",
        "- Do you need to tune other generation configurations? (do_sample, temperature , top_p, top_k)\n",
        "- Do you think complexity of shots and domain relationship would make a difference?\n",
        "- Do you think the similarity between the model training data and your task plays a role here?\n",
        "- Does it works with others tasks?\n",
        "\n",
        "### Let's know what you think in the comments...\n",
        "\n"
      ],
      "metadata": {
        "id": "9wKBwnw9y6oH"
      },
      "id": "9wKBwnw9y6oH"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1zbGMt1HqYiP"
      },
      "id": "1zbGMt1HqYiP",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "availableInstances": [
      {
        "_defaultOrder": 0,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.t3.medium",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 1,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.t3.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 2,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.t3.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 3,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.t3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 4,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 5,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 6,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 7,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 8,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 9,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 10,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 11,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 12,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5d.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 13,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5d.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 14,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5d.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 15,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5d.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 16,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5d.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 17,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5d.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 18,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5d.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 19,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 20,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": true,
        "memoryGiB": 0,
        "name": "ml.geospatial.interactive",
        "supportedImageNames": [
          "sagemaker-geospatial-v1-0"
        ],
        "vcpuNum": 0
      },
      {
        "_defaultOrder": 21,
        "_isFastLaunch": true,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.c5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 22,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.c5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 23,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.c5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 24,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.c5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 25,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 72,
        "name": "ml.c5.9xlarge",
        "vcpuNum": 36
      },
      {
        "_defaultOrder": 26,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 96,
        "name": "ml.c5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 27,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 144,
        "name": "ml.c5.18xlarge",
        "vcpuNum": 72
      },
      {
        "_defaultOrder": 28,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.c5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 29,
        "_isFastLaunch": true,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g4dn.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 30,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g4dn.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 31,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g4dn.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 32,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g4dn.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 33,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g4dn.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 34,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g4dn.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 35,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 61,
        "name": "ml.p3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 36,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 244,
        "name": "ml.p3.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 37,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 488,
        "name": "ml.p3.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 38,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.p3dn.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 39,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.r5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 40,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.r5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 41,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.r5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 42,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.r5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 43,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.r5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 44,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.r5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 45,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.r5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 46,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.r5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 47,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 48,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 49,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 50,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 51,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 52,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 53,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.g5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 54,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.g5.48xlarge",
        "vcpuNum": 192
      },
      {
        "_defaultOrder": 55,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 56,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4de.24xlarge",
        "vcpuNum": 96
      }
    ],
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}