{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ✍️ Prompt Engineering Techniques with LLMs : From Zero to Hero 🤖💡\n",
        "\n",
        "Welcome to this hands-on tutorial on **Prompt Engineering** — the art and science of crafting inputs to get the best out of Large Language Models (LLMs)! In this notebook, we’ll explore multiple prompt strategies like **zero-shot**, **one-shot**, and **few-shot** prompting to guide model behavior in different tasks.\n",
        "\n",
        "👉 While the examples here use **LLaMA 2 7B Chat** models, the concepts apply to **any modern LLM**, such as GPT, Claude, or Mistral.\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 What You'll Learn:\n",
        "- 🧠 What is prompt engineering and why it matters\n",
        "- 🔁 How to format prompts for chat-based models (System, User, Assistant)\n",
        "- 🧪 Different prompt styles: zero-shot, one-shot, and few-shot learning\n",
        "- 🧩 Prompting for summarization, translation, reasoning, and more\n",
        "\n",
        "This tutorial is perfect for **developers, data scientists, and AI enthusiasts** who want to get practical with LLMs — no fine-tuning required! 💬\n"
      ],
      "metadata": {
        "id": "iuXcioMr7X-U"
      },
      "id": "iuXcioMr7X-U"
    },
    {
      "cell_type": "markdown",
      "id": "57fee1e7-081e-4c22-af70-9839fcfcec32",
      "metadata": {
        "id": "57fee1e7-081e-4c22-af70-9839fcfcec32"
      },
      "source": [
        "# Prompt Engineering Techniques 🚀\n",
        "\n",
        "Let’s explore the exciting world of **Prompt Engineering** using Meta’s LLaMA 2 **`LLaMA-2-7B-chat`**, a powerful model by Meta with **7 billion parameters**, fine-tuned specifically for conversational tasks and instruction following.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3336b23c-be24-4299-82d1-4ba0c578385d",
      "metadata": {
        "id": "3336b23c-be24-4299-82d1-4ba0c578385d"
      },
      "source": [
        "### 🌟 Import Helper Functions\n",
        "\n",
        "To get started, let’s import all the necessary Python libraries and suppress any unwanted warnings. This sets the stage for using the Together API smoothly!\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 👉 Install the Together Python client\n",
        "!pip install Together"
      ],
      "metadata": {
        "id": "wETTeb3jV447"
      },
      "id": "wETTeb3jV447",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 👉 Import necessary Python libraries\n",
        "import requests                     # for making API requests\n",
        "import os                           # for accessing environment variables\n",
        "import json                         # for working with JSON data\n",
        "import warnings                     # for suppressing warnings\n",
        "from google.colab import userdata   # Colab utility to access user secrets\n",
        "import time                         # for adding delays if needed\n",
        "\n",
        "# 👉 Ignore warnings to keep output clean\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "0n1V6xBw5gW5"
      },
      "id": "0n1V6xBw5gW5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🤝 About Together AI — A Platform Powering Open-Source AI at Scale\n",
        "\n",
        "**Together AI** (together.ai) is a cutting-edge AI cloud platform designed to help developers, researchers, and businesses **train, tune, and serve generative AI models**—especially those that are open-source.  \n",
        "\n",
        "### ⚙️ What Can You Do with Together AI?\n",
        "\n",
        "- **Inference at Scale**: Deploy models like LLaMA, Qwen, or your own fine‑tuned models using high-performance **serverless or dedicated API endpoints**. Optimized for speed and cost efficiency.\n",
        "- **Fine-Tuning & Adaptation**: Customize open-source models using your own data with APIs that support both lightweight adapters (LoRA) and full fine‑tuning pipelines.   \n",
        "- **GPU Clusters & Training Infrastructure**: Access NVIDIA H100, H200, GB200 GPUs across scalable clusters for training or inference, with enterprise-grade orchestration and scheduling.\n",
        "\n",
        "### 🔒 Enterprise & Deployment Options\n",
        "\n",
        "- Offers both **fully-managed cloud** deployment and **VPC/private deployment** for organizations with strong security and privacy needs.\n",
        "- Built to comply with enterprise standards like **SOC‑2 and HIPAA**, ensuring your data and models remain under your control.\n",
        "\n",
        "### 📈 Why It Matters for Prompt Engineering\n",
        "\n",
        "- **Easy APIs** allow seamless integration into notebooks or apps without deep infrastructure setup.\n",
        "- **Context injection & knowledge updates** become simple: you can fine-tune or supply fresh data to your model on-demand.\n",
        "- **Cost‑effective performance** — Together claims **up to 4× faster inference** and **significantly lower costs** compared to many other platforms.  \n",
        "\n",
        "### 🚀 About the Company\n",
        "\n",
        "Founded in 2022 and based in San Francisco, Together AI is backed by major investors including Salesforce Ventures, NVIDIA, and General Catalyst.\n",
        "\n",
        "### Dive into your prompts with confidence—Powered by Together!\n"
      ],
      "metadata": {
        "id": "4iLBD3c34lK2"
      },
      "id": "4iLBD3c34lK2"
    },
    {
      "cell_type": "code",
      "source": [
        "# 👉 Import the Together API client\n",
        "from together import Together\n",
        "\n",
        "# 👉 Initialize the client\n",
        "client = Together()\n",
        "\n",
        "# 👉 Define a simple user message for sentiment analysis\n",
        "response = client.chat.completions.create(\n",
        "    model=\"meta-llama/Llama-3.2-3B-Instruct-Turbo\",  # specify the model\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"\n",
        "        Analyze the sentiment of the following message:\n",
        "        Hey Lina, I truly appreciated your support during the event!\n",
        "        \"\"\"\n",
        "      }\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 👉 Print the model's response\n",
        "print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "id": "B4DoYAmRV2rE"
      },
      "id": "B4DoYAmRV2rE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🚀 Setting Up the API Endpoint & Authentication\n",
        "\n",
        "Now that we’ve imported all the necessary libraries, it’s time to prepare the connection to the **Together API**.\n",
        "\n",
        "Here’s what we’ll do:\n",
        "- Define the API **endpoint URL** 🌐\n",
        "- Add an **Authorization header** using your secret API key 🔐 (from Google Colab's `userdata` storage)\n",
        "- Set the content type to **JSON** 🧾\n",
        "\n",
        "This setup ensures that we can send prompts securely to the hosted model and receive structured responses.\n"
      ],
      "metadata": {
        "id": "6Tv3g0zg473O"
      },
      "id": "6Tv3g0zg473O"
    },
    {
      "cell_type": "code",
      "source": [
        "# 👉 Define the Together Inference API endpoint\n",
        "url = \"https://api.together.xyz/inference\"\n",
        "\n",
        "# 👉 Setup the headers for authorization using your API key\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {userdata.get('TOGETHER_API_KEY')}\",  # Automatically grabs the key from Google Colab secrets\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "ZhRGtvXWYn-p"
      },
      "id": "ZhRGtvXWYn-p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🧰 Let’s Build a Prompt Sender Function!\n",
        "\n",
        "To make our workflow smooth and reusable, we’ll define a **helper function** that sends prompts to the Together API.\n",
        "\n",
        "Here’s what this function will handle:\n",
        "- 🧠 Specify which model to use (default is LLaMA 3.2B)\n",
        "- ✍️ Accept both a `system prompt` (model behavior) and a `user prompt` (instruction)\n",
        "- 📬 Send the request and fetch the model’s reply\n",
        "\n",
        "This will save us from repeating boilerplate code and help us focus on experimenting with different prompt strategies!\n"
      ],
      "metadata": {
        "id": "rLbRPV6k5Pks"
      },
      "id": "rLbRPV6k5Pks"
    },
    {
      "cell_type": "code",
      "source": [
        "# 👉 Define a helper function to interact with the LLaMA model\n",
        "def generate_llama_response(system_prompt, user_prompt, model=\"meta-llama/Llama-3.2-3B-Instruct-Turbo\"):\n",
        "    # Create the prompt structure with roles\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},  # System prompt defines context\n",
        "            {\"role\": \"user\", \"content\": user_prompt}       # User prompt is the actual instruction\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    # Send a POST request to the Together inference API\n",
        "    response = requests.post(url, headers=headers, data=json.dumps(payload))\n",
        "\n",
        "    # Extract the generated content from the response\n",
        "    result = response.json()\n",
        "    return result['output']['choices'][0]['text']"
      ],
      "metadata": {
        "id": "G2bjx8ZA5UUp"
      },
      "id": "G2bjx8ZA5UUp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function simplifies the process of interacting with the LLaMA model. You just need to provide a system prompt (context for the assistant) and a user prompt (actual task), and it will return the generated response. Neat, right? 🙌"
      ],
      "metadata": {
        "id": "ewDFhXUmKSTf"
      },
      "id": "ewDFhXUmKSTf"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_llama_param(prompt,\n",
        "          model= \"meta-llama/Llama-3.2-3B-Instruct-Turbo\",\n",
        "          #model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
        "          #model= \"meta-llama/Llama-3-8b-chat-hf\",\n",
        "          #model=\"togethercomputer/llama-2-7b-chat\",\n",
        "          temperature=0.0,\n",
        "          max_tokens=256,\n",
        "          verbose=False,\n",
        "          url=url,\n",
        "          headers=headers):\n",
        "\n",
        "    data = {\n",
        "            \"model\": model,\n",
        "            \"prompt\": prompt,\n",
        "            \"temperature\": temperature,\n",
        "            \"max_tokens\": max_tokens\n",
        "        }\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    return response.json()['output']['choices'][0]['text']\n"
      ],
      "metadata": {
        "id": "9WQmixVB46eJ"
      },
      "id": "9WQmixVB46eJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🧠 In-Context Learning (ICL) — Let the Model Learn from Examples!\n",
        "\n",
        "🧩 Instead of just telling the model what to do, you **show it a few examples** in your prompt. It then figures out the pattern and continues based on what it learned from those examples!\n",
        "\n",
        "💬 It’s like showing a friend how to solve 2 math problems and asking them to solve the third one.\n"
      ],
      "metadata": {
        "id": "tMX10qnx5iVz"
      },
      "id": "tMX10qnx5iVz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✏️ Try It: Standard Instruction Prompt (No Examples Yet)\n",
        "\n",
        "Let’s start with a simple example of **standard instruction prompting**.\n",
        "\n",
        "In this case, we won’t show the model any examples. Instead, we’ll give it a clear, direct instruction and ask it to perform a task.\n",
        "\n",
        "Think of it like saying:\n",
        "🗣️ “Hey assistant, translate this sentence into Spanish.”\n",
        "\n",
        "Let’s try that using the `generate_llama_response` function we just defined!\n"
      ],
      "metadata": {
        "id": "cK3foptE5uZX"
      },
      "id": "cK3foptE5uZX"
    },
    {
      "cell_type": "code",
      "source": [
        "# 👉 Give the model an explicit instruction without any examples\n",
        "instruction = \"Translate the following sentence to Spanish:\"\n",
        "sentence = \"Where is the nearest train station?\"\n",
        "\n",
        "# 👉 Generate and print the model's response\n",
        "response = generate_llama_response(\"You are a helpful assistant.\", f\"{instruction}\\n{sentence}\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "3nFSEbRb50pa"
      },
      "id": "3nFSEbRb50pa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ea534c7-e213-417f-a99c-0f8dba92387e",
      "metadata": {
        "id": "3ea534c7-e213-417f-a99c-0f8dba92387e"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Analyze the sentiment of the following message:\n",
        "Hey Lina, I truly appreciated your support during the event!\n",
        "\"\"\"\n",
        "response = generate_llama_param(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45096c64-f483-4d01-b64c-8de1fb529441",
      "metadata": {
        "id": "45096c64-f483-4d01-b64c-8de1fb529441"
      },
      "source": [
        "### 🎯 Zero-Shot Prompting — Make a Guess with No Hints!\n",
        "\n",
        "Now let’s test **Zero-Shot Prompting**. This means:\n",
        "❌ No examples  \n",
        "❌ No explicit task definition  \n",
        "✅ Just structure — and let the model infer what to do!\n",
        "\n",
        "It’s like handing someone a form that says:\n",
        "📄 `\"Dog: Chien Cat: Chat Bird: \"` — and expecting them to figure out the translation task.\n",
        "\n",
        "🧠 This is useful when:\n",
        "• You want to test the model’s general intelligence  \n",
        "• The task is obvious from context  \n",
        "• You want fewer tokens (cheaper & faster!)\n",
        "\n",
        "Let’s give it a shot!\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 👉 Zero-shot translation: Let the model infer the task from the pattern\n",
        "zero_shot_prompt = \"\"\"\n",
        "English: Car\n",
        "French:\"\"\"\n",
        "\n",
        "# 👉 Call the model with no instruction, just the pattern\n",
        "response = generate_llama_response(\"You are a translation assistant.\", zero_shot_prompt)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "Orxbz4-kLeWt"
      },
      "id": "Orxbz4-kLeWt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should see the French word for \"Car\" — which is “Voiture”! 🚗🇫🇷\n",
        "\n",
        "If you add examples to following, it will be few shots.\n",
        "\n",
        "English: Apple<br>\n",
        "French: Pomme<br>\n",
        "English: House<br>\n",
        "French: Maison<br>\n",
        "English: Car<br>\n",
        "French:"
      ],
      "metadata": {
        "id": "Jiw4u_tFLhlL"
      },
      "id": "Jiw4u_tFLhlL"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "549667d6-6b23-46a9-8c4a-f089f7936392",
      "metadata": {
        "id": "549667d6-6b23-46a9-8c4a-f089f7936392"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Message: Hey Lina, I truly appreciated your support during the event!\n",
        "Sentiment:\n",
        "\"\"\"\n",
        "response = generate_llama_param(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b162fb3d-7020-47c6-bb08-5ed6f138da66",
      "metadata": {
        "id": "b162fb3d-7020-47c6-bb08-5ed6f138da66"
      },
      "source": [
        "### 🧪 Few-Shot Prompting — Teaching with Examples\n",
        "\n",
        "Few-shot prompting means we give the model a **few examples** of the task in the prompt before asking it to complete a similar one.\n",
        "\n",
        "👉 It helps the model “learn” what we expect by showing real use cases.  \n",
        "👉 Great for more complex or custom tasks where zero-shot might not be enough.\n",
        "\n",
        "🧠 Think of it like saying:\n",
        "> \"Here are 3 problems and their solutions — now solve the next one.\"\n",
        "\n",
        "Let’s show it how to convert measurements, and then ask it to convert another!\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 👉 Construct a few-shot prompt with several examples\n",
        "few_shot_prompt = \"\"\"\n",
        "Convert the following measurements from inches to centimeters:\n",
        "\n",
        "5 inches = 12.7 cm\n",
        "12 inches = 30.48 cm\n",
        "8 inches = 20.32 cm\n",
        "15 inches =\n",
        "\"\"\"\n",
        "\n",
        "# 👉 Generate the model's prediction\n",
        "response = generate_llama_response(\"You are a measurement converter.\", few_shot_prompt)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "FKL3UrDeL9KL"
      },
      "id": "FKL3UrDeL9KL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🎯 What to Expect:\n",
        "If the model understood the examples, it should reply:\n",
        "“38.1 cm” because 15 inches × 2.54 = 38.1 cm!"
      ],
      "metadata": {
        "id": "FEAFznTcMAzC"
      },
      "id": "FEAFznTcMAzC"
    },
    {
      "cell_type": "markdown",
      "id": "41560853-9639-4329-87ac-640620bb4b39",
      "metadata": {
        "id": "41560853-9639-4329-87ac-640620bb4b39"
      },
      "source": [
        "### Specifying the Output Format\n",
        "- You can also specify the format in which you want the model to respond.\n",
        "- In the example below, you are asking to \"give a one word response\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f2669f8-9673-4568-aec2-4b9f13d033ba",
      "metadata": {
        "id": "4f2669f8-9673-4568-aec2-4b9f13d033ba"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Message: You forgot our dinner reservation again!\n",
        "Sentiment: Negative\n",
        "\n",
        "Message: Looking forward to the beach trip this weekend!\n",
        "Sentiment: Positive\n",
        "\n",
        "Message: Hey Lina, I truly appreciated your support during the event!\n",
        "Sentiment:\n",
        "\n",
        "Responsd in one word only.\n",
        "\"\"\"\n",
        "response = generate_llama_param(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6e07176-5730-4cc0-bec1-cb3ae9000806",
      "metadata": {
        "id": "a6e07176-5730-4cc0-bec1-cb3ae9000806"
      },
      "source": [
        "Let's try bigger model, for example: `llama-2-70b-chat` model..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "415739fe-dcfe-4183-a5d8-fa3ce19aa480",
      "metadata": {
        "id": "415739fe-dcfe-4183-a5d8-fa3ce19aa480"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Message: You forgot our dinner reservation again!\n",
        "Sentiment: Negative\n",
        "\n",
        "Message: Looking forward to the beach trip this weekend!\n",
        "Sentiment: Positive\n",
        "\n",
        "Message: Hey Lina, I truly appreciated your support during the event!\n",
        "Sentiment:\n",
        "\n",
        "Respond with only: positive, negative, or neutral.\n",
        "\"\"\"\n",
        "response = generate_llama_param(prompt, model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe196c59-2c42-4bdb-b7e7-a2f431ffce29",
      "metadata": {
        "id": "fe196c59-2c42-4bdb-b7e7-a2f431ffce29"
      },
      "source": [
        "- Now we use the smaller model while\n",
        "restricting the output format to choose from `positive`, `negative` or `neutral`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74528daa-90be-4208-98ac-3cf71dc687b6",
      "metadata": {
        "id": "74528daa-90be-4208-98ac-3cf71dc687b6"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Message: Hi Dad, you're 20 minutes late to my piano recital!\n",
        "Sentiment: Negative\n",
        "\n",
        "Message: Can't wait to order pizza for dinner tonight\n",
        "Sentiment: Positive\n",
        "\n",
        "Message: Hi Amit, thanks for the thoughtful birthday card!\n",
        "Sentiment:\n",
        "\n",
        "Respond with either positive, negative, or neutral.\n",
        "\"\"\"\n",
        "# Don't add any other text.\n",
        "response = generate_llama_param(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47d3d067-2b77-4a35-b95b-d7ed9c5bc77f",
      "metadata": {
        "id": "47d3d067-2b77-4a35-b95b-d7ed9c5bc77f"
      },
      "source": [
        "### 🎭 Role Prompting — Set the Stage for the Model\n",
        "\n",
        "Here’s a fun and creative prompting trick — **Role Prompting**!\n",
        "\n",
        "By assigning the model a role or persona, you can guide its behavior in specific ways:\n",
        "🧑‍🏫 \"You are a history professor...\"  \n",
        "👩‍💼 \"You are a career advisor...\"  \n",
        "🧙 \"You are a fantasy storyteller...\"\n",
        "\n",
        "✨ Why it works:\n",
        "Giving the model a role changes its **`tone`**, **`vocabulary`**, and **`response structure`**. It's like an actor putting on a costume 🎭 — and acting accordingly!\n",
        "\n",
        "Let’s try asking the model to act like a poetic Shakespearean assistant!\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 👉 Ask the model to respond in a poetic, Shakespearean style\n",
        "role_prompt = \"\"\"\n",
        "You are a Shakespearean assistant.\n",
        "User: Tell me about Artificial Intelligence.\n",
        "\"\"\"\n",
        "\n",
        "# 👉 Let’s see how creatively the model can respond!\n",
        "response = generate_llama_response(\"You are a wise assistant from the 1600s.\", role_prompt)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "Z3NuZnf3MrvK"
      },
      "id": "Z3NuZnf3MrvK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🎭 Expected Output:\n",
        "Something like: “O wondrous machine of mindless thought, that mimiceth man’s wit and toil...”\n",
        "\n",
        "🧠 Notice how the model adjusts its style and tone based on the role you gave it!"
      ],
      "metadata": {
        "id": "1fy8LHj2MvOO"
      },
      "id": "1fy8LHj2MvOO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2db7b960-d6f6-48a0-8c62-a68c999ad4fd",
      "metadata": {
        "id": "2db7b960-d6f6-48a0-8c62-a68c999ad4fd"
      },
      "outputs": [],
      "source": [
        "role = \"\"\"\n",
        "You are a motivational speaker who shares wisdom in a poetic and inspiring style.\n",
        "You respond with clarity, warmth, and hope.\n",
        "\"\"\"\n",
        "\n",
        "prompt = f\"\"\"\n",
        "{role}\n",
        "How can I respond to this question from a colleague:\n",
        "What does it mean to live a fulfilling life?\n",
        "\"\"\"\n",
        "response = generate_llama_param(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51371f2d-9204-41b1-930c-2bd99888c5ab"
      },
      "source": [
        "### 🔗 Chain-of-Thought Prompting (CoT) — Teach the Model to “Think Out Loud” 🧠💬\n",
        "\n",
        "Sometimes, we don’t want only the answer — we want also the **reasoning behind it**.\n",
        "\n",
        "That’s where Chain-of-Thought (CoT) prompting shines. Instead of simply asking for an answer, we **guide the model to break down its thinking process** step by step.\n",
        "\n",
        "💡 Think of it like showing your work in math class:\n",
        "> “First, we calculate X, then we multiply by Y… therefore, the final answer is Z.”\n",
        "\n",
        "### ✅ Why is this helpful?\n",
        "- Encourages **logical reasoning**\n",
        "- Improves **accuracy** on complex problems\n",
        "- Produces **transparent and explainable** outputs\n",
        "- Boosts performance on tasks involving **math, logic, and multi-step decisions**\n",
        "\n",
        "Let’s put this into action with a real example.\n"
      ],
      "id": "51371f2d-9204-41b1-930c-2bd99888c5ab"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Q: If a train travels 60 miles in 1 hour, how far will it travel in 3 hours?\n",
        "A: Let's think step by step.\n",
        "\"\"\"\n",
        "\n",
        "print(generate_llama_response(\"You are a smart math tutor.\", prompt))\n"
      ],
      "metadata": {
        "id": "1jsUxUeQQjyZ"
      },
      "execution_count": null,
      "outputs": [],
      "id": "1jsUxUeQQjyZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "🎯 What to Expect:\n",
        "\n",
        "\"The train travels 60 miles in 1 hour. In 3 hours, it will travel 60 × 3 = 180 miles. So the answer is 180 miles.\"\n",
        "\n",
        "📌 Takeaway: By explicitly telling the model to “think step by step,” we encourage a sequential and correct line of reasoning."
      ],
      "metadata": {
        "id": "E74GmjHIQlXl"
      },
      "id": "E74GmjHIQlXl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔄 Let’s Refine the Prompt — Add More Instruction\n",
        "\n",
        "CoT works best when the model is gently nudged to explain its thought process *before* answering.\n",
        "\n",
        "So let’s rephrase the instruction more clearly to ask the model to \"explain before answering.\"\n",
        "\n",
        "This is perfect when:\n",
        "- You want to **see how the model thinks**\n",
        "- You need **intermediate steps**\n",
        "- You want the **user to trust the answer**\n",
        "\n",
        "Let’s try this improved version with another example.\n"
      ],
      "metadata": {
        "id": "Cs8zkVc-Qpbk"
      },
      "id": "Cs8zkVc-Qpbk"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Q: If there are 12 apples and 4 people, how many apples does each person get?\n",
        "A: Please think step by step before answering.\n",
        "\"\"\"\n",
        "\n",
        "print(generate_llama_response(\"You are a patient problem solver.\", prompt))\n"
      ],
      "metadata": {
        "id": "lrAkkFclQr6C"
      },
      "execution_count": null,
      "outputs": [],
      "id": "lrAkkFclQr6C"
    },
    {
      "cell_type": "markdown",
      "source": [
        "✅ What you should see:\n",
        "\n",
        "\"There are 12 apples and 4 people. We divide 12 by 4. Each person gets 3 apples.\"\n",
        "\n",
        "🧠 Note: This level of clarity is essential in educational tools, tutoring platforms, and even business logic applications."
      ],
      "metadata": {
        "id": "fvMPE8sYQyT9"
      },
      "id": "fvMPE8sYQyT9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🧾 Advanced Tip: Separate the Reasoning from the Final Answer\n",
        "\n",
        "In real-world use cases (like legal reasoning, tutoring, or decision-making), we often want to:\n",
        "- ✅ Read the logic first\n",
        "- ✅ Then see a clean, final answer at the end\n",
        "\n",
        "This format mimics **how humans explain decisions**. It’s also great when you're chaining model outputs together and need a clear delimiter for the next step.\n",
        "\n",
        "Let’s apply this to a simple money calculation.\n"
      ],
      "metadata": {
        "id": "sSHON5rHQ0bq"
      },
      "id": "sSHON5rHQ0bq"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Q: A toy costs $15. If you buy 4 toys, how much will you spend?\n",
        "\n",
        "Explain your reasoning first, then state the final answer at the end.\n",
        "\"\"\"\n",
        "\n",
        "print(generate_llama_response(\"You are an analytical assistant.\", prompt))\n"
      ],
      "metadata": {
        "id": "5XNAwMKoQ3ZV"
      },
      "execution_count": null,
      "outputs": [],
      "id": "5XNAwMKoQ3ZV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "📘 Expected:\n",
        "\n",
        "“Each toy costs $15. 4 × 15 = $60. Final Answer: $60”\n",
        "\n",
        "🚀 Pro move: You can even ask the model to return the final answer in a specific format (e.g., Answer: $60) for parsing in apps."
      ],
      "metadata": {
        "id": "4FP8YvVUQ2vV"
      },
      "id": "4FP8YvVUQ2vV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ⚠️ The Order of Instructions Matters! (Seriously!)\n",
        "\n",
        "LLMs generate text **token by token, left to right** — so the **way you phrase and order your instructions** affects the outcome.\n",
        "\n",
        "This means:\n",
        "- “Think step by step, then answer” 🟢 works well\n",
        "- “Give me the answer first, then explain” 🔴 might skip reasoning entirely\n",
        "\n",
        "Always put **reasoning first**, answer second — unless you want the reverse.\n",
        "\n",
        "Let’s reinforce this idea with one last example.\n"
      ],
      "metadata": {
        "id": "qKIYMmeYQ9Yq"
      },
      "id": "qKIYMmeYQ9Yq"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Q: If a car uses 2 gallons of gas per hour and drives for 5 hours, how much gas does it use?\n",
        "\n",
        "First, think through the problem carefully. Then, and only then, give the final number of gallons used.\n",
        "\"\"\"\n",
        "\n",
        "print(generate_llama_response(\"You are a thoughtful and careful AI assistant.\", prompt))\n"
      ],
      "metadata": {
        "id": "uq2TJ2MKRD9F"
      },
      "execution_count": null,
      "outputs": [],
      "id": "uq2TJ2MKRD9F"
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧮 Expected Output:\n",
        "\n",
        "“The car uses 2 gallons/hour. In 5 hours, 2 × 5 = 10 gallons. Final Answer: 10 gallons.”\n",
        "\n",
        "🧠 Why this works: You're training the model — via prompt — to pause and process before blurting out a result."
      ],
      "metadata": {
        "id": "xbqsYpISRGJe"
      },
      "id": "xbqsYpISRGJe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🧠 Final Thought on Chain-of-Thought Prompting\n",
        "\n",
        "Here’s a mind-blowing fact:  \n",
        "Large Language Models like LLaMA **don’t actually “think.”** They predict the **next word** based on your input — that’s it.\n",
        "\n",
        "But when you tell them to **“think step by step”**, you’re giving them a pattern that **simulates reasoning**.\n",
        "\n",
        "And that’s the secret of Chain-of-Thought prompting:\n",
        "> You trick the model into reasoning — by modeling what reasoning looks like.\n",
        "\n",
        "This technique is used in:\n",
        "- 🧠 Complex reasoning chains\n",
        "- 🧮 Math problem-solving\n",
        "- 🤖 Autonomous agents (like AutoGPT)\n",
        "- 💡 Multi-turn reasoning chatbots\n",
        "\n",
        "So the next time you want smarter, more thoughtful answers — just say:\n",
        "👉 “Let’s think step by step.”  \n",
        "...and watch the magic happen. ✨\n"
      ],
      "metadata": {
        "id": "wSUKpDBXRJSZ"
      },
      "id": "wSUKpDBXRJSZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5756ee56-eaa7-43ab-8531-4f575014b26a"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "We have 30 people going to a conference.\n",
        "Three vans are available.\n",
        "Each van fits 6 people.\n",
        "One person has a motorbike for 2.\n",
        "\n",
        "Can we transport everyone using only these vehicles?\n",
        "\n",
        "First, think step by step. Then provide a one word answer: Yes or No.\n",
        "\"\"\"\n",
        "\n",
        "response = generate_llama_response(\"You are a group leader\",prompt)\n",
        "print(response)"
      ],
      "id": "5756ee56-eaa7-43ab-8531-4f575014b26a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42594189-5692-4ab3-9a1b-3d6ac58c8a78"
      },
      "source": [
        "### 🤖 Combining Techniques — Let's Chain Prompts Together!\n",
        "\n",
        "Now that we've learned **zero-shot**, **few-shot**, **chain-of-thought**, and **role prompting**, let’s combine techniques for more control.\n",
        "\n",
        "In this next example, we’ll:<BR>\n",
        "• Assign a role  \n",
        "• Use reasoning steps  \n",
        "• End with a clear instruction\n",
        "\n",
        "This helps the model deliver better answers with more context and structure — a real prompt engineering win! 🏆\n"
      ],
      "id": "42594189-5692-4ab3-9a1b-3d6ac58c8a78"
    },
    {
      "cell_type": "code",
      "source": [
        "# 👉 Combine role prompting with reasoning for better control\n",
        "combined_prompt = \"\"\"\n",
        "You are a helpful teaching assistant. Answer the question and explain why.\n",
        "\n",
        "Q: Why do birds migrate in winter?\n",
        "A:\n",
        "\"\"\"\n",
        "\n",
        "# 👉 The model should now answer and justify its reasoning\n",
        "response = generate_llama_response(\"You are helpful, kind, and intelligent.\", combined_prompt)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "Yxheyw8WM59P"
      },
      "id": "Yxheyw8WM59P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🌍 What to Look For:\n",
        "A thoughtful answer like:\n",
        "\n",
        "“Birds migrate in winter to find food and warmer climates. As temperatures drop, food becomes scarce...”"
      ],
      "metadata": {
        "id": "HpuV0GDiM-jU"
      },
      "id": "HpuV0GDiM-jU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 💥 Bonus: Prompt for Creative Writing\n",
        "\n",
        "Prompt engineering isn't just for math and Q&A — it can spark creativity too! 🎨\n",
        "\n",
        "Let’s use a prompt to generate a story intro and test how well the model handles open-ended narrative writing. This shows how LLMs can be used in:<br>\n",
        "• Storytelling apps  \n",
        "• Script writing  \n",
        "• Game design and NPC behavior  \n",
        "\n",
        "Ready to get creative? 🌈\n"
      ],
      "metadata": {
        "id": "DxfOmGK4NClg"
      },
      "id": "DxfOmGK4NClg"
    },
    {
      "cell_type": "code",
      "source": [
        "# 👉 Prompt the model to write a story introduction\n",
        "story_prompt = \"\"\"\n",
        "You are a fantasy storyteller. Begin a tale with the following line:\n",
        "\n",
        "\"In the heart of the forgotten forest, a single lantern flickered...\"\n",
        "\"\"\"\n",
        "\n",
        "response = generate_llama_response(\"You are a master of fantasy storytelling.\", story_prompt)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "ZQxQofWvNEVh"
      },
      "id": "ZQxQofWvNEVh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📚 Expected Result:\n",
        "An engaging story beginning with vivid imagery, characters, and magical setting!"
      ],
      "metadata": {
        "id": "pGuG8I5MNHWE"
      },
      "id": "pGuG8I5MNHWE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🧪 Testing the Model with Factual Recall\n",
        "\n",
        "Let’s see how the model performs when we prompt it with factual questions — like a quiz! 🧠\n",
        "\n",
        "This time, we’ll just ask a direct question to see if the model can recall the correct fact from its training data.\n",
        "\n",
        "This is great for:<br>\n",
        "• Trivia-style chatbots  \n",
        "• Educational assistants  \n",
        "• Verifying general knowledge\n",
        "\n",
        "Let’s try asking about a Nobel Prize winner!\n"
      ],
      "metadata": {
        "id": "Ngj2OHo7Nfcz"
      },
      "id": "Ngj2OHo7Nfcz"
    },
    {
      "cell_type": "code",
      "source": [
        "# 👉 Ask the model a factual question about Nobel Prizes\n",
        "#factual_prompt = \"Who won the Nobel Prize in Literature in 2021?\"\n",
        "factual_prompt = \"Who won the Nobel Prize in Physics in 2024?\"\n",
        "\n",
        "response = generate_llama_response(\"You are an expert in global awards and honors.\", factual_prompt)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "eaRF84EHNGxv"
      },
      "id": "eaRF84EHNGxv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 Tip:\n",
        "The correct answer is Abdulrazak Gurnah, but depending on the model's training cutoff, it might guess incorrectly.\n",
        "⚠️ Always double-check factual outputs when using LLMs in knowledge-based apps!"
      ],
      "metadata": {
        "id": "54cqB_YdNlQf"
      },
      "id": "54cqB_YdNlQf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d32b6c3-c49d-4297-b58d-65bfceeda14e"
      },
      "source": [
        "### 🧠 Providing New Information in the Prompt — When the Model Doesn't Know\n",
        "\n",
        "LLMs like LLaMA are trained on data up to a certain cutoff date. So, if something happened after that — like a recent sports event or election — the model **won’t know** unless you **tell it in the prompt**.\n",
        "\n",
        "Let’s see what happens if we ask about a recent event the model likely wasn’t trained on...\n"
      ],
      "id": "9d32b6c3-c49d-4297-b58d-65bfceeda14e"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Who won the Wimbledon Championship in 2024?\n",
        "\"\"\"\n",
        "\n",
        "print(generate_llama_response(\"You are a sports expert.\", prompt))\n"
      ],
      "metadata": {
        "id": "ActGkbPVPnJE"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ActGkbPVPnJE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🤔 What If the Model Doesn’t Know? (Dealing with Outdated Knowledge)\n",
        "\n",
        "Here’s something important to remember:  \n",
        "Large Language Models like LLaMA are trained on data from the **past** — they don’t know what happened **after their training cutoff date**. 📆\n",
        "\n",
        "So if you ask:\n",
        "> “Who won the Wimbledon Championship in 2024?”\n",
        "\n",
        "...and the model was trained before that event, it might **guess**, say **it doesn’t know**, or even give a wrong answer like that I Mohamed did win the championship! 😅\n",
        "\n",
        "Let’s test it out and see how it responds when we give it **no extra information**.\n"
      ],
      "metadata": {
        "id": "0EVlZisJPtBn"
      },
      "id": "0EVlZisJPtBn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1e29e6b-706e-438c-8d56-39bbf23cb94c"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "Who won the 2024 Men's UEFA European Championship?\n",
        "\"\"\"\n",
        "response = generate_llama_param(prompt)\n",
        "print(response)"
      ],
      "id": "f1e29e6b-706e-438c-8d56-39bbf23cb94c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "💡 Context injection is great for:\n",
        "- Recent events 🗞️\n",
        "- Company-specific knowledge 🏢\n",
        "- Custom workflows or datasets 🧠\n",
        "\n",
        "Now, let’s define our context!\n"
      ],
      "metadata": {
        "id": "G2QbcR5_P4_-"
      },
      "id": "G2QbcR5_P4_-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f959e08-e682-4ec0-8401-c4ed4d791485"
      },
      "source": [
        "- As you can see, the model still thinks that the tournament is yet to be played, even though you are now in 2024!\n",
        "- Another thing to **note** is, July 18, 2023 was the date the model was released to public, and it was trained even before that, so it only has information upto that point. The response says, \"the final match is scheduled to take place in July 2023\", but the final match was played on August 20, 2023."
      ],
      "id": "5f959e08-e682-4ec0-8401-c4ed4d791485"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30874cc3-73d7-412a-9dc5-1baa6aabf1b7"
      },
      "source": [
        "- You can provide the model with information about recent events, in this case text from Wikipedia about the 2023 Women's World Cup."
      ],
      "id": "30874cc3-73d7-412a-9dc5-1baa6aabf1b7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "context = \"\"\"\n",
        "The 2024 Wimbledon Championship was won by Carlos Alcaraz in the men’s singles category,\n",
        "and Iga Świątek won the women’s singles title, both displaying incredible performances in the final matches.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "nK14mNaGP95G"
      },
      "id": "nK14mNaGP95G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 This is the factual snippet we want the model to read before answering."
      ],
      "metadata": {
        "id": "kwAf7kJtQBvC"
      },
      "id": "kwAf7kJtQBvC"
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"\"\" The 2024 Wimbledon Championship was won by Carlos Alcaraz in the men’s singles category,\n",
        "and Iga Świątek won the women’s singles title, both displaying incredible performances in the final matches. \"\"\"\n",
        "prompt = f\"\"\"\n",
        "{context}\n",
        "\n",
        "Q: Who won the Wimbledon Championship in 2024?\n",
        "A:\n",
        "\"\"\"\n",
        "\n",
        "print(generate_llama_response(\"You are a sports expert who keeps up with the latest news.\", prompt))\n"
      ],
      "metadata": {
        "id": "FBEavr00QECi"
      },
      "execution_count": null,
      "outputs": [],
      "id": "FBEavr00QECi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔢 Expected Output and Explanation\n",
        "📣 Expected Result:\n",
        "\n",
        "“Carlos Alcaraz and Iga Świątek won the 2024 Wimbledon Championship in men’s and women’s singles respectively.”\n",
        "\n",
        "🎯 Why this matters:\n",
        "By injecting the latest facts into the prompt, you can overcome the model’s training limitations and get timely, accurate answers — without retraining the model!\n",
        "\n",
        "This is prompt engineering magic. ✨"
      ],
      "metadata": {
        "id": "AIteAruYQIa-"
      },
      "id": "AIteAruYQIa-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cbe1b18-be2a-4f77-9911-c243e571226c"
      },
      "outputs": [],
      "source": [
        "context = \"\"\"\n",
        "The 2023 Nobel Peace Prize was awarded to Narges Mohammadi for her courageous fight against the oppression of women in Iran\n",
        "and her efforts to promote human rights and freedom for all. Despite being imprisoned multiple times for her activism,\n",
        "she continued to be a voice for change, advocating for the abolition of the death penalty and highlighting the conditions of political prisoners.\n",
        "The Nobel Committee recognized her as a symbol of the broader movement for women’s rights and democratic reform in the region. Her award follows\n",
        "the 2022 prize, which was shared by human rights defenders in Belarus, Russia, and Ukraine.\n",
        "\"\"\"\n"
      ],
      "id": "1cbe1b18-be2a-4f77-9911-c243e571226c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8b9d517-23f8-4468-9fc7-e47322d5b2fa"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "Based on the context provided, who won the 2023 Nobel Peace Prize?\n",
        "context: {context}\n",
        "\"\"\"\n",
        "response = generate_llama_param(prompt)\n",
        "print(response)"
      ],
      "id": "c8b9d517-23f8-4468-9fc7-e47322d5b2fa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 📧 Prompting for Emails — Business Use Case\n",
        "\n",
        "Prompt engineering is super helpful in professional settings too!  \n",
        "You can ask the model to:\n",
        "\n",
        "• Write emails  \n",
        "• Create reports  \n",
        "• Summarize long messages  \n",
        "• Generate customer responses\n",
        "\n",
        "Let’s test a prompt where the model has to write a kind but firm email reply.\n"
      ],
      "metadata": {
        "id": "sIzynG8FNoRs"
      },
      "id": "sIzynG8FNoRs"
    },
    {
      "cell_type": "code",
      "source": [
        "# 👉 Ask the model to draft a polite and professional email\n",
        "email_prompt = \"\"\"\n",
        "Write a professional email declining a job offer due to a better opportunity, while expressing appreciation.\n",
        "\"\"\"\n",
        "\n",
        "response = generate_llama_response(\"You are a professional HR assistant.\", email_prompt)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "c9q3Wa0YNqal"
      },
      "id": "c9q3Wa0YNqal",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📬 What to Expect:\n",
        "A thoughtful email like:\n",
        "\n",
        "“Thank you very much for the opportunity. After careful consideration, I have decided to pursue another position that aligns more closely with my goals…”"
      ],
      "metadata": {
        "id": "p7FMpnBQNs1N"
      },
      "id": "p7FMpnBQNs1N"
    },
    {
      "cell_type": "markdown",
      "id": "c0169c84-8f68-4544-8ee1-d02265635d49",
      "metadata": {
        "id": "c0169c84-8f68-4544-8ee1-d02265635d49"
      },
      "source": [
        "### 📝 Text Summarization — Let the Model Do the Reading for You!\n",
        "\n",
        "Let’s wrap up with one of the most **practical and popular use cases** for LLMs — **summarization**! 🎯\n",
        "\n",
        "Instead of reading long articles or reports, we can ask the model to:<BR>\n",
        "• 🔍 Pull out the key ideas  \n",
        "• 📌 Create concise bullet points  \n",
        "• 🧠 Generate TL;DR summaries\n",
        "\n",
        "Summarization is powerful in:<BR>\n",
        "• Productivity tools  \n",
        "• News aggregators  \n",
        "• Customer support platforms  \n",
        "• Academic or research environments\n",
        "\n",
        "Let’s try summarizing a short paragraph now!\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 👉 Define a paragraph to summarize\n",
        "text_to_summarize = \"\"\"\n",
        "Large language models like LLaMA are revolutionizing the way humans interact with machines.\n",
        "They can understand natural language, generate meaningful responses, and even simulate reasoning.\n",
        "These capabilities make them valuable for a wide range of applications including education, research, business, and entertainment.\n",
        "\"\"\"\n",
        "\n",
        "# 👉 Prompt the model to create a summary\n",
        "summary_prompt = f\"Summarize the following text:\\n\\n{text_to_summarize}\"\n",
        "\n",
        "response = generate_llama_response(\"You are a professional summarizer.\", summary_prompt)\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "nfMM0yZXOCyb"
      },
      "id": "nfMM0yZXOCyb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧾 Expected Output Example:\n",
        "\n",
        "\"LLaMA models enable human-like interactions with machines and are useful in education, research, business, and entertainment.\"\n",
        "\n",
        "🎯 Why This Matters:\n",
        "Summarization helps compress information and improve decision-making by getting to the point fast. You can customize the tone, format (e.g., bullets), or length — making it flexible and efficient for real-world tasks."
      ],
      "metadata": {
        "id": "XNJyeI_IOFkc"
      },
      "id": "XNJyeI_IOFkc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "434cd16f-ef7f-4a6b-9076-007893848518",
      "metadata": {
        "id": "434cd16f-ef7f-4a6b-9076-007893848518"
      },
      "outputs": [],
      "source": [
        "email = \"\"\"\n",
        "Hi Alex,\n",
        "\n",
        "I hope you're doing well. I wanted to bring you up to speed on our latest discussions from the AI strategy sync held on Monday.\n",
        "\n",
        "We’ve decided to begin transitioning our customer support chatbot to use a Retrieval-Augmented Generation (RAG) architecture. This approach will allow the system to retrieve up-to-date information from our internal knowledge base and inject it into the prompt context, significantly improving response accuracy. We're currently experimenting with LLaMA 3 (8B and 70B) through Together.ai’s hosted endpoints due to their latency optimizations and cost flexibility. Hugging Face inference endpoints remain our fallback.\n",
        "\n",
        "On the legal tech side, we’ve concluded that few-shot prompting has reached its limit for document summarization. We’ll move toward fine-tuning a smaller model (likely Mistral-7B) using a labeled corpus of 4,000+ legal documents to improve reliability and reduce token consumption. This aligns with our broader cost-reduction goals while maintaining domain-specific quality.\n",
        "\n",
        "Another key point: we're evaluating model hosting options on AWS Bedrock and Azure ML. Initial tests on Bedrock with Anthropic's Claude and Amazon’s Titan models have shown promising stability, but GPU allocation on Azure appears to be more scalable in bursts. We'll continue benchmarking with production-like load simulations.\n",
        "\n",
        "We're also allocating responsibilities for implementation:\n",
        "- Priya and Omar will lead the RAG pipeline integration.\n",
        "- Lina will coordinate dataset preparation and fine-tuning workflows.\n",
        "- I'll work with DevOps to assess deployment bottlenecks and CI/CD automation for model rollouts.\n",
        "\n",
        "Next steps:\n",
        "1. Finalize model selection for chatbot and summarization by Friday.\n",
        "2. Set up retrieval infrastructure and embedding index next week.\n",
        "3. Prepare compliance review for data used in fine-tuning.\n",
        "\n",
        "Let’s schedule a deep-dive call next Wednesday to align timelines and allocate resources.\n",
        "\n",
        "Best regards,\n",
        "Jordan\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0664cba1-437d-4073-9877-b34ff96335f1",
      "metadata": {
        "id": "0664cba1-437d-4073-9877-b34ff96335f1"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "Summarize this internal strategy email and extract the team's key AI decisions.\n",
        "What architectural choices were made for the chatbot and legal summarization use cases?\n",
        "\n",
        "email: {email}\n",
        "\"\"\"\n",
        "\n",
        "response = generate_llama_param(prompt)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What We Learned About Prompt Engineering\n",
        "\n",
        "Congrats on making it through this hands-on tutorial! You’ve just explored a powerful skill set that will supercharge how you work with LLMs like LLaMA. 🚀\n",
        "\n",
        "### 📚 Here's a recap of the prompt engineering strategies we covered:\n",
        "\n",
        "🔹 **Zero-Shot Prompting** — No examples, just a task  \n",
        "🔹 **Few-Shot Prompting** — Add examples to teach the model  \n",
        "🔹 **In-Context Learning** — Let the model generalize from patterns  \n",
        "🔹 **Chain-of-Thought (CoT)** — Ask it to explain its thinking  \n",
        "🔹 **Role Prompting** — Give it a persona for more natural responses  \n",
        "🔹 **Creative & Factual Prompts** — From storytelling to real-world questions  \n",
        "🔹 **Business Use Cases** — Writing emails, summaries, and more\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "6Izj8WWENzuV"
      },
      "id": "6Izj8WWENzuV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Final Wrap-Up: Mastering Prompt Engineering with LLaMA 🧠✨\n",
        "\n",
        "Congratulations on reaching the end of this hands-on journey! 🚀  \n",
        "You’ve just explored the **art and science of prompt engineering**, and gained a powerful new skill to unlock the potential of Large Language Models like LLaMA. 💡\n",
        "\n",
        "Let’s take a moment to reflect on everything we’ve learned:\n",
        "\n",
        "---\n",
        "\n",
        "### 🧰 Prompt Engineering Techniques You Now Master\n",
        "\n",
        "#### 🔹 1. **Zero-Shot Prompting**\n",
        "Ask a question without examples.  \n",
        "✅ Great for simple, well-known tasks.\n",
        "\n",
        "> 🧪 “Translate this sentence into French.”  \n",
        "> 🎯 Direct and fast.\n",
        "\n",
        "---\n",
        "\n",
        "#### 🔹 2. **Few-Shot Prompting**\n",
        "Provide a few labeled examples.  \n",
        "✅ Ideal when the task is uncommon or has nuance.\n",
        "\n",
        "> Example:  \n",
        "> “2+2 = 4, 3+5 = 8… Now solve 7+6.”\n",
        "\n",
        "---\n",
        "\n",
        "#### 🔹 3. **In-Context Learning**\n",
        "Let the model **learn from patterns** embedded in your prompt.  \n",
        "✅ Excellent for custom logic or formats.\n",
        "\n",
        "> Teach with examples, then ask it to continue the pattern.\n",
        "\n",
        "---\n",
        "\n",
        "#### 🔹 4. **Output Formatting**\n",
        "Ask for structured results like:\n",
        "- Bullet points\n",
        "- Tables\n",
        "- JSON\n",
        "- Step-by-step reasoning\n",
        "\n",
        "✅ Perfect for APIs, dashboards, and pipelines.\n",
        "\n",
        "---\n",
        "\n",
        "#### 🔹 5. **Role Prompting**\n",
        "Assign the model a persona or job title.  \n",
        "✅ Controls tone, style, and vocabulary.\n",
        "\n",
        "> “You are a Shakespearean assistant…” 🎭  \n",
        "> “You are a professional executive coach…” 👔\n",
        "\n",
        "---\n",
        "\n",
        "#### 🔹 6. **Context Injection**\n",
        "When the model doesn’t know something (e.g., Wimbledon 2024 results), give it the answer yourself.\n",
        "\n",
        "✅ This technique helps:\n",
        "- Overcome training cut-off dates\n",
        "- Add real-time or domain-specific info\n",
        "- Improve factuality\n",
        "\n",
        "---\n",
        "\n",
        "#### 🔹 7. **Summarization**\n",
        "Ask the model to condense long content into short insights.  \n",
        "✅ Essential for productivity, education, and knowledge management.\n",
        "\n",
        "> TL;DR the CEO’s email into 1 sentence ✅  \n",
        "> Summarize a Wikipedia article in 3 bullet points ✅\n",
        "\n",
        "---\n",
        "\n",
        "#### 🔹 8. **Chain-of-Thought Prompting**\n",
        "The *crown jewel* of prompt engineering.  \n",
        "Ask the model to “think step by step” and watch its reasoning unfold.\n",
        "\n",
        "✅ Great for:\n",
        "- Math problems\n",
        "- Logic puzzles\n",
        "- Transparent decision-making\n",
        "- Agent workflows\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Key Takeaways\n",
        "\n",
        "✅ Prompt engineering isn’t just about *what* you ask — it’s about *how* you ask.\n",
        "✅ Simple changes in phrasing or structure can significantly improve output.\n",
        "✅ You’re not “programming” the model — you’re **collaborating** with it. 🤝\n",
        "✅ This is a creative and iterative process. Test, tweak, and try again!\n",
        "✅ Small changes in your prompt can lead to BIG improvements in output  \n",
        "✅ Context, clarity, and creativity make a difference  \n",
        "✅ Prompt engineering is part art, part science — so experiment boldly!\n",
        "\n",
        "---\n",
        "### ⚠️ Common Mistakes to Avoid\n",
        "\n",
        "- ❌ Asking multiple tasks in one prompt without structure\n",
        "- ❌ Forgetting to guide output format\n",
        "- ❌ Assuming the model knows post-2023 facts\n",
        "- ❌ Skipping “think step by step” in reasoning prompts\n",
        "\n",
        "Fix them by:\n",
        "✅ Breaking instructions into steps  \n",
        "✅ Injecting necessary context  \n",
        "✅ Using role and tone consistently\n",
        "\n",
        "### 🎯 What's Next?\n",
        "\n",
        "💡 Try deploying these prompts in your own apps  \n",
        "💡 Explore prompt tuning or fine-tuning if you want even more control  \n",
        "💡 Check out more models like GPT-NeoX, Claude, or Mistral for comparison\n",
        "💡 Build smarter chatbots  \n",
        "💡 Summarize long documents  \n",
        "💡 Create structured outputs for apps  \n",
        "💡 Design multi-step reasoning chains  \n",
        "💡 Inject knowledge into the model on-the-fly  \n",
        "💡 Build products powered by LLaMA or other LLMs\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 🙌 Thanks for Learning With Me!\n",
        "\n",
        "If this tutorial helped you:\n",
        "- Share it with a friend 💌\n",
        "- Try out your own prompts and build something cool 🛠️\n",
        "- Explore more advanced topics like prompt tuning or fine-tuning 🔧\n",
        "\n",
        "Keep experimenting. Keep learning. Keep prompting.  \n",
        "And remember — the smartest AI… is the one you *prompt* well. 😉  \n",
        "\n",
        "**💬 Let’s think step by step — and build the future together.**\n"
      ],
      "metadata": {
        "id": "SNEmFR5SRctT"
      },
      "id": "SNEmFR5SRctT"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QjgF7n-ON0Np"
      },
      "id": "QjgF7n-ON0Np",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}