{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üß† Text Classification with Hugging Face ü§ó\n"
      ],
      "metadata": {
        "id": "fMFLBdDrpfWz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOuVHW1FkzAX"
      },
      "source": [
        "Welcome! üëã In this notebook, we‚Äôll explore how to **fine-tune a DistilBERT Transformer model** for **emotion classification** using the powerful ü§ó Hugging Face ecosystem.\n",
        "\n",
        "By the end, you‚Äôll be able to:\n",
        "- Load and understand a real-world emotion dataset\n",
        "- Tokenize text for Transformers\n",
        "- Fine-tune `DistilBERT` on labeled text\n",
        "- Evaluate and predict with your custom model\n",
        "- Load our fine tuned model to Hugging Face Hub ü§ó\n",
        "\n",
        "Let‚Äôs get started!\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment the following lines if you're running this notebook locally\n",
        "# and haven't already installed Hugging Face Transformers and Datasets\n",
        "\n",
        "#!pip install transformers==4.13.0 #datasets==2.8.0\n",
        "!pip install -U datasets huggingface_hub fsspec"
      ],
      "metadata": {
        "id": "RIcbIymfp4MD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wh4C6Owk9CG"
      },
      "source": [
        "## üìÇ Step 1: Load the Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the ü§ó `datasets` library ‚Äî a powerful and flexible tool for accessing, inspecting, and preprocessing datasets directly from the Hugging Face Hub.\n",
        "> It allows for efficient streaming, filtering, and preprocessing ‚Äî all within your notebook!\n",
        "\n",
        "\n",
        "Each dataset is identified by a unique name. We'll start by loading the `emotion` dataset.\n"
      ],
      "metadata": {
        "id": "cleN7dQTqI7t"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otzfIqQoK5X1"
      },
      "source": [
        "Let‚Äôs now load the `emotion` dataset using `load_dataset()`. This dataset contains short English text samples (tweets) labeled with one of six emotions like \"joy\", \"anger\", or \"sadness\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RzmOwugBS3O"
      },
      "source": [
        "üßæ **About the Dataset**:  \n",
        "- It contains 3 splits: `train`, `validation`, and `test`\n",
        "- Each example is a tweet labeled with one of six basic emotions\n",
        "- The `label` field is an integer index mapped to emotion names\n",
        "\n",
        "Perfect for learning how to fine-tune language models for classification tasks!\n",
        "\n",
        "\n",
        "Each example has two fields:\n",
        "- `text`: the tweet\n",
        "- `label`: an integer mapped to an emotion\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gtl6_57yK5X1"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the emotion dataset from Hugging Face Hub\n",
        "emotionDataset = load_dataset(\"emotion\")\n",
        "\n",
        "# Print type and summary of the dataset\n",
        "print(\"üì¶ Data Set Type:\", type(emotionDataset))\n",
        "print(emotionDataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXXA1PfMK5X2"
      },
      "source": [
        "üí° We import the load_dataset() function and fetch the \"emotion\" dataset. Notice it returns a dictionary-like object with keys for each split: train, validation, and test.\n",
        "\n",
        "Let‚Äôs inspect what we just loaded üìä\n",
        "\n",
        "The `emotionDataset` object behaves like a Python dictionary, with keys corresponding to each split (`train`, `validation`, `test`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can access each split like a normal dictionary:\n",
        "\n",
        "```python\n",
        "emotionTrainSet = emotionDataset[\"train\"]\n"
      ],
      "metadata": {
        "id": "z4yRu0evKxi8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå Inspect the Training Set (Code)\n"
      ],
      "metadata": {
        "id": "Znd4bcl1LBth"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cuTqaVG5YQ7X"
      },
      "outputs": [],
      "source": [
        "# Get the training split\n",
        "emotionTrainSet = emotionDataset[\"train\"]\n",
        "\n",
        "# View dataset summary\n",
        "print(emotionTrainSet)\n",
        "print(\"üìä Size of Training Set:\", emotionTrainSet.num_rows)\n",
        "print(\"üß± Column Names:\", emotionTrainSet.column_names)\n",
        "print(\"üîç Column Features:\", emotionTrainSet.features)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéØ This gives us the first few samples in the training set ‚Äî Every sample is a tweet and its corresponding label."
      ],
      "metadata": {
        "id": "ZOlJX12_I1qu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAa3ldfLxQ7D"
      },
      "outputs": [],
      "source": [
        "# Preview the first training example\n",
        "print(emotionTrainSet[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWBjlDj0xbjY"
      },
      "outputs": [],
      "source": [
        "# View first 5 text samples (tweets)\n",
        "print(emotionTrainSet[:5][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxFDS-y3xkyK"
      },
      "outputs": [],
      "source": [
        "print(emotionTrainSet[:5][\"label\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† This gives us the raw input the model will learn from after transformation. We will see this in the follwoing cells.\n",
        "\n",
        "üéØ Each number represents a class (like 0 = sadness, 1 = joy, etc.)\n",
        "\n"
      ],
      "metadata": {
        "id": "OwXKKXPHLpBC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ9Q3tU2K5YG"
      },
      "source": [
        "## üß™ Step 2: From Text to Tokens\n",
        "\n",
        "Before feeding text into our Transformer model, we need to tokenize it.\n",
        "\n",
        "Tokenization breaks the text down into smaller pieces ‚Äî and in Transformers, we use **subword tokenization** which offers a nice balance between vocabulary size and flexibility.\n",
        "\n",
        "There are three common types of tokenization:\n",
        "- üî§ Character-level: one token per character\n",
        "- üß± Word-level: one token per word\n",
        "- üß¨ Subword-level (used in BERT/DistilBERT): flexible hybrid method\n",
        "\n",
        "‚û°Ô∏è We‚Äôll use Hugging Face‚Äôs `AutoTokenizer` to convert text to token IDs.\n",
        "\n",
        "üß† This tokenizer converts raw text into token IDs using a pretrained vocabulary and includes special tokens for padding, classification, and separation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEY0ojKz3ROC"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "# Set the pretrained model checkpoint\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "\n",
        "# Load tokenizer using AutoTokenizer (recommended)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Alternative: use task-specific tokenizer class\n",
        "#distilbert_tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)\n",
        "\n",
        "# Display tokenizer configuration and metadata\n",
        "print(\"üß† Tokenizer Vocabulary Size: \", tokenizer.vocab_size)\n",
        "print(\"ü™ü Max Input Length (Context Window): \", tokenizer.model_max_length)\n",
        "print(\"üîñ Special Token IDs: \", tokenizer.all_special_ids)\n",
        "print(\"üî§ Special Token Names: \", tokenizer.all_special_tokens)\n",
        "print(\"üìé Model Inputs: \", tokenizer.model_input_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå Tokenize a Sample Sentence"
      ],
      "metadata": {
        "id": "2Bj1RPTUM4lj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e52decjW5_JH"
      },
      "outputs": [],
      "source": [
        "# Example sentence\n",
        "text = \"This tutorial will show you Hugging Face Models can be fine tuned using transformers and PyTorch!\"\n",
        "\n",
        "# Tokenize the sentence into IDs and structure\n",
        "encodedText = tokenizer(text)\n",
        "\n",
        "# Print the encoded structure\n",
        "print(\"üßæ Text Encoded:\", encodedText)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîç We see how the tokenizer breaks the sentence into IDs and adds metadata like attention masks.\n",
        "\n",
        "üìå Let's isualize Tokens.. This helps us verify how the model sees the text ‚Äî including [CLS] and [SEP] tokens"
      ],
      "metadata": {
        "id": "rKVIxIaXNkvQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DM2seYWC5DuG"
      },
      "outputs": [],
      "source": [
        "# Convert token IDs back into actual tokens\n",
        "\n",
        "print(\"Tokens: \",tokenizer.convert_ids_to_tokens(encodedText[\"input_ids\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIB6ECI-5FmW"
      },
      "outputs": [],
      "source": [
        "# Reconstruct the original sentence from token IDs\n",
        "\n",
        "decodedText = tokenizer.decode(encodedText[\"input_ids\"])\n",
        "print(\"üîÅ Text Decoded:\", decodedText)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéØ This confirms that the tokenizer can reverse the process ‚Äî useful for debugging model outputs"
      ],
      "metadata": {
        "id": "v0uwWqq5N5U_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aVyOq29K5YW"
      },
      "source": [
        "### üóÇÔ∏è Tokenizing the Entire Dataset\n",
        "\n",
        "To prepare our model for training, we need to tokenize all the text in our dataset. We'll do this efficiently using the `map()` function from Hugging Face Datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_CrJHWhSrse"
      },
      "outputs": [],
      "source": [
        "# Reset the dataset format in case it was modified earlier\n",
        "#‚ö†Ô∏èThis clears any previous formatting (e.g., PyTorch/TensorFlow formatting) and resets to default.\n",
        "emotionDataset.reset_format()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VHMhWA-SvTJ"
      },
      "outputs": [],
      "source": [
        "# Define a function that tokenizes batches of text\n",
        "\n",
        "def tokenizeBatch(batch):\n",
        "  return tokenizer(batch[\"text\"], padding=True, truncation=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† We pad and truncate to make input lengths uniform ‚Äî critical for batch processing.\n",
        "\n",
        "üîé Let's verify that our tokenizer works on multiple examples and outputs the expected fields."
      ],
      "metadata": {
        "id": "NEHmS0xGOPLG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-MO0v1LTRGw"
      },
      "outputs": [],
      "source": [
        "# Preview the raw text batch\n",
        "print(\"üìù Original Text Batch:\\n\", emotionDataset[\"train\"][:4])\n",
        "\n",
        "# Apply the tokenization function to that batch\n",
        "print(\"üî¢ Tokenized Text Batch:\\n\", tokenizeBatch(emotionDataset[\"train\"][:4]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJV0tzpYFOAV"
      },
      "source": [
        "Let‚Äôs now tokenize the **entire dataset**.  \n",
        "üöÄ Using `map()` with `batched=True` allows efficient parallel processing across dataset entries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQhdcFnHUJBN"
      },
      "outputs": [],
      "source": [
        "# Apply the tokenization function to all dataset splits\n",
        "\n",
        "emotionsDatasetEncoded = emotionDataset.map(tokenizeBatch, batched=True, batch_size=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå The tokenized dataset includes new fields like input_ids and attention_mask, which are required by Transformers."
      ],
      "metadata": {
        "id": "wgmx3IEePs6C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ecjmqe2PUgBD"
      },
      "outputs": [],
      "source": [
        "# Print original and tokenized dataset columns\n",
        "print(\"üìã Original Dataset Columns:\", emotionDataset[\"train\"].column_names)\n",
        "print(\"üìã Dataset Columns:\", emotionsDatasetEncoded[\"train\"].column_names)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZIV9gRiF_pE"
      },
      "source": [
        "### üßæ What Does the Tokenizer Return?\n",
        "\n",
        "After tokenization, each example now contains:\n",
        "- `input_ids`: the token ID sequence\n",
        "- `attention_mask`: tells the model which tokens are actual input (1) and which are padding (0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut-JZNiZK5Ya"
      },
      "source": [
        "## üèóÔ∏è Step 3: Training a Text Classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvWGI3lNHyMe"
      },
      "source": [
        "ü§ñ `DistilBERT` model is trained for **masked language modeling** and not classification.  \n",
        "So we need to modify the model and use a classification head.\n",
        "\n",
        "There are two main options when adapting pretrained models for classification:\n",
        "\n",
        "- üîç **Feature Extraction**:  \n",
        "  Freeze the pretrained model and use its hidden states to train a separate classifier (like transfer learning in CNNs).\n",
        "\n",
        "- üîß **Fine-Tuning**:  \n",
        "  Train the entire model end-to-end ‚Äî including the pretrained layers.\n",
        "\n",
        "> We‚Äôll use **fine-tuning** in this notebook for better accuracy.\n",
        "\n",
        "‚úÖ **Pro Tip**: You can also explore **PEFT (Parameter-Efficient Fine-Tuning)** methods like LoRA, AdapterFusion, or BitFit to reduce compute and memory cost.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß© Option 1: Load the Pretrained DistilBERT Model\n",
        "\n",
        "We‚Äôll now load the base DistilBERT model using Hugging Face‚Äôs `AutoModel` class.\n",
        "\n",
        "> This gives us access to the model's raw hidden states, without any classification head. üîç Using AutoModel instead of AutoModelForSequenceClassification lets us access the model's internal representations (hidden states) directly.\n"
      ],
      "metadata": {
        "id": "4vKV00qtRNsS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "voJYAQIZUunJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModel\n",
        "\n",
        "# Check if a GPU is available; fallback to CPU if not\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"üìü Device chosen:\", device)\n",
        "\n",
        "# Load the pretrained DistilBERT model and move it to the appropriate device\n",
        "#Remember model checkpoint: distilber-base-uncased\n",
        "model = AutoModel.from_pretrained(model_checkpoint).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üß† Extracting the Last Hidden States\n",
        "\n",
        "Let‚Äôs warm up by retrieving the **last hidden states** from DistilBERT for a single input text.\n",
        "\n",
        "To do this, we must convert our input text into **PyTorch-compatible tensors** using the tokenizer.\n",
        "\n",
        "> This is done by passing `return_tensors=\"pt\"` to the tokenizer.\n"
      ],
      "metadata": {
        "id": "8PueuIl8R0-p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToXO3x8hWP3r"
      },
      "outputs": [],
      "source": [
        "# Print the original example sentence\n",
        "print(\"üìù Original example text:\", text)\n",
        "\n",
        "# Preview its tokenized version (ID format)\n",
        "print(\"üî¢ Tokenized as ID map:\", encodedText)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MPedF7xn1Fl"
      },
      "outputs": [],
      "source": [
        "# Tokenize again, but return as PyTorch tensors\n",
        "encodedTextPT = tokenizer(text, return_tensors=\"pt\")\n",
        "\n",
        "# Print input tensor shape and structure\n",
        "print(\"üìê Encoded tensor shape:\", encodedTextPT[\"input_ids\"].size())\n",
        "print(\"üì¶ Tokenized Tensor Dictionary:\", encodedTextPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üí° Now our input is in the format expected by PyTorch models ‚Äî a dictionary of tensors."
      ],
      "metadata": {
        "id": "nc6iYMX0Swse"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DK6BJO_orqw"
      },
      "outputs": [],
      "source": [
        "# Function to get hidden states from a batch input\n",
        "def get_hidden_state(batch):\n",
        "    # Only keep keys the model needs (input_ids, attention_mask)\n",
        "    inputs = {k:v.to(device) for k,v in batch.items() if k in tokenizer.model_input_names} #Model Input Names (Only what model needs):\n",
        "\n",
        "    #Disable gradient tracking for inference\n",
        "    with torch.no_grad():\n",
        "        last_hidden_state = model(**inputs).last_hidden_state\n",
        "\n",
        "    #return last_hidden_state\n",
        "\n",
        "    # Return only the [CLS]-like token representation (first token)\n",
        "    # [CLS]: include attention to all tokens\n",
        "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}\n",
        "\n",
        "# Test on our single input\n",
        "print(\"üì§ DistilBERT Hidden State Output:\\n\", get_hidden_state(encodedTextPT))\n",
        "\n",
        "#print(get_hidden_state(encodedTextPT).size())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† Extracting the embedding of the first token [CLS] is commonly used to represent the entire sequence in classification applications. The reason is that, the embeddings concise attention to all other tokens in the sequence.\n",
        "\n",
        "üîç Tensor Dimensions Breakdown\n",
        "\n",
        "The output shape of `last_hidden_state` is: [batch_size, sequence_length, embedding_dimension]\n",
        "\n",
        "\n",
        "Where:\n",
        "- `batch_size`: number of inputs in a batch\n",
        "- `sequence_length`: number of tokens per input\n",
        "- `embedding_dimension`: typically 768 for DistilBERT\n",
        "\n",
        "We use `torch.no_grad()` to disable gradient calculations (since we‚Äôre just extracting values, not training).\n"
      ],
      "metadata": {
        "id": "qkcKDWkKT33K"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PXlj4qGK5Yj"
      },
      "source": [
        "### üí° Apply to All Data\n",
        "\n",
        "Let‚Äôs now compute the hidden states for the entire dataset. We‚Äôll use `map()` to apply our `get_hidden_state()` function to every example.\n",
        "\n",
        "> This creates a new column called `hidden_state` in each dataset split.\n",
        "\n",
        "üîÑ Prepare Dataset for PyTorch\n",
        "\n",
        "Before applying the model, we need to convert the dataset columns into PyTorch tensors.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oluz_P4rt0I1"
      },
      "outputs": [],
      "source": [
        "# Convert key columns to torch.Tensor for compatibility with model input\n",
        "emotionsDatasetEncoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "# Confirm new format\n",
        "emotionsDatasetEncoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inKxOrrErRUf"
      },
      "outputs": [],
      "source": [
        "# üìå Print a training example's inputs and label\n",
        "print(\"üìú Text Example:\", emotionsDatasetEncoded['train']['text'][0])\n",
        "print(\"üè∑Ô∏è Label:\", emotionsDatasetEncoded['train']['label'][0])\n",
        "print(\"üî¢ Input IDs:\", emotionsDatasetEncoded['train']['input_ids'][0])\n",
        "print(\"üé≠ Attention Mask:\", emotionsDatasetEncoded['train']['attention_mask'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let‚Äôs apply `get_hidden_state()` to all examples using `batched=True` for faster performance.\n",
        "\n",
        "> ‚è±Ô∏è **Note**: Running the next cell on CPU can be slow. GPU acceleration is highly recommended.\n"
      ],
      "metadata": {
        "id": "p6ZBvWbKZ44e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nyQdoDQsgFk"
      },
      "outputs": [],
      "source": [
        "# Apply hidden state extraction to all dataset splits using batched processing\n",
        "emotionsDatasetEncoded= emotionsDatasetEncoded.map(get_hidden_state, batched=True)\n",
        "\n",
        "# Show structure\n",
        "emotionsDatasetEncoded"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† `batch_size=None` here means that the dataset library used its default value, which is `batch_size=1000` during the `map()` operation.\n",
        "\n",
        "This batch size is used when applying our custom function (like `get_hidden_state`) across examples.\n",
        "\n",
        "\n",
        "üöÄ Each example now includes a new hidden_state vector ‚Äî a numerical representation of its text, ready for downstream tasks like classification.\n",
        "\n",
        "üìå We confirm that the dataset now contains hidden representations (hidden_state) alongside the raw and tokenized text."
      ],
      "metadata": {
        "id": "it2q1KUUWBW-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfYM63WNsrqy"
      },
      "outputs": [],
      "source": [
        "# View available columns in the tokenized dataset\n",
        "print(\"üßæ Column Names:\", emotionsDatasetEncoded.column_names)\n",
        "\n",
        "\n",
        "# Preview the first example in the training set\n",
        "print(\"üìù Text:\", emotionsDatasetEncoded['train']['text'][0])\n",
        "print(\"üè∑Ô∏è Label:\", emotionsDatasetEncoded['train']['label'][0])\n",
        "print(\"üî¢ Input IDs:\", emotionsDatasetEncoded['train']['input_ids'][0])\n",
        "print(\"üé≠ Attention Mask:\", emotionsDatasetEncoded['train']['attention_mask'][0])\n",
        "print(\"üîÆ Hidden State Vector:\", emotionsDatasetEncoded['train']['hidden_state'][0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sdlhn86GK5Yk"
      },
      "source": [
        " `batch_size=None` in this case, so the default `batch_size=1000`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA6xVobeK5Yl"
      },
      "source": [
        "### üß± Creating a Feature Matrix for Classification\n",
        "\n",
        "Now that we‚Äôve generated vector representations for each sentence, we‚Äôre ready to train a classifier.\n",
        "\n",
        "We‚Äôll use these hidden states as **input features** and the emotion labels as **targets**.\n",
        "\n",
        "The encoded dataset already contains everything we need.\n",
        "\n",
        "We'll convert the hidden state vectors and their corresponding labels into NumPy arrays ‚Äî perfect for training models in `Scikit-learn`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g95Ztk6vtNsh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Extract features (X) and labels (y) for training and validation\n",
        "emotionXTrain = np.array(emotionsDatasetEncoded[\"train\"][\"hidden_state\"])\n",
        "emotionXVal = np.array(emotionsDatasetEncoded[\"validation\"][\"hidden_state\"])\n",
        "\n",
        "emotionYTrain = np.array(emotionsDatasetEncoded[\"train\"][\"label\"])\n",
        "emotionYVal = np.array(emotionsDatasetEncoded[\"validation\"][\"label\"])\n",
        "\n",
        "# Display the shapes of the resulting matrices\n",
        "print(\"üìê Training set shape:\", emotionXTrain.shape)\n",
        "print(\"üìê Validation set shape:\", emotionXVal.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìä We now have numerical matrices ready to feed into traditional classifiers like `Logistic Regression`.\n",
        "\n"
      ],
      "metadata": {
        "id": "NoHar-RkXQPR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eN612ixVuKcc"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Train logistic regression on the sentence embeddings\n",
        "lr_classifier = LogisticRegression(max_iter=3000)\n",
        "lr_classifier.fit(emotionXTrain, emotionYTrain)\n",
        "\n",
        "# Evaluate model on validation set\n",
        "print(\"‚úÖ LR Validation Accuracy:\",lr_classifier.score(emotionXVal, emotionYVal))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üéØ This simple model gives us a strong baseline using only DistilBERT's embeddings ‚Äî without fine-tuning!\n",
        "#### ü§ñ Performance Comparison with a Dummy Classifier\n",
        "\n",
        "Let‚Äôs compare our logistic regression model with a baseline model that **always predicts the most frequent class**.\n",
        "\n",
        "üìå This sets a low baseline ‚Äî any model doing better than this is actually learning something!\n"
      ],
      "metadata": {
        "id": "JgVe32whXg0Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2LASd45u4Ap"
      },
      "outputs": [],
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "# Create and fit a baseline dummy classifier\n",
        "dmy_classifier = DummyClassifier(strategy=\"most_frequent\")\n",
        "dmy_classifier.fit(emotionXTrain, emotionYTrain)\n",
        "\n",
        "# Evaluate dummy classifier accuracy\n",
        "print(\"üìâ Dummy Classifier Validation Accuracy:\", dmy_classifier.score(emotionXVal, emotionYVal))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÅ Option 2: Fine-Tuning the Transformer Model Directly\n",
        "\n",
        "Instead of using sentence embeddings, we can **fine-tune the entire DistilBERT model** end-to-end for classification.\n",
        "\n",
        "üîç Why use a neural network head?\n",
        "\n",
        "To make the classification head trainable with the Transformer model, we need a **differentiable output layer**.\n",
        "\n",
        "This is why we typically append a neural network classification head to the base model ‚Äî allowing gradient-based learning.\n"
      ],
      "metadata": {
        "id": "wMJKcxJWYBwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ‚ö° Benefits of Fine-Tuning\n",
        "\n",
        "Fine-tuning allows the model to:\n",
        "- Adapt its internal weights to the target task\n",
        "- Improve performance on domain-specific data\n",
        "- Potentially surpass the performance of frozen-feature approaches\n",
        "\n",
        "This is especially powerful when working with large pretrained models on smaller datasets.\n"
      ],
      "metadata": {
        "id": "54qKLXDDYZE5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîß Load a Pretrained Model for Classification\n",
        "\n",
        "We‚Äôll now use `AutoModelForSequenceClassification` instead of `AutoModel`.\n",
        "\n",
        "This gives us:\n",
        "- A pretrained DistilBERT backbone\n",
        "- A classification head on top\n",
        "\n",
        "You only need to specify the number of output labels (in our case, six).\n",
        "\n",
        "The `AutoModelForSequenceClassification` class wraps a base model like DistilBERT with a **trainable classification layer**.\n",
        "\n",
        "> It‚Äôs ideal for supervised tasks like sentiment analysis, spam detection, or ‚Äî in our case ‚Äî **emotion classification**.\n"
      ],
      "metadata": {
        "id": "zTNMAe14Yknw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoeJnGXCyAE9"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Define the number of output labels ‚Äî we have 6 emotion classes\n",
        "#'sadness', 'joy', 'love', 'anger', 'fear', 'surprise'\n",
        "num_labels =6\n",
        "\n",
        "# Load the DistilBERT model with a classification head for sequence classification\n",
        "tf_classifier = (AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels).to(device))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† This loads DistilBERT and attaches a randomly initialized classification head ‚Äî the part that we‚Äôll fine-tune!\n",
        "> ‚ö†Ô∏è You might see a warning that some model weights were randomly initialized ‚Äî that‚Äôs expected!  \n",
        "The base DistilBERT model is pretrained, but the classification head is new and will be trained from scratch during fine-tuning.\n"
      ],
      "metadata": {
        "id": "eQTH7mW2ZG97"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xBfZZXaK5Yt"
      },
      "source": [
        "To monitor training progress, we‚Äôll define a custom `compute_metrics()` üìè function.  \n",
        "This is used by Hugging Face‚Äôs `Trainer` to calculate metrics like accuracy and F1-score during evaluation.\n",
        "\n",
        "The function receives predictions and labels from the model and returns a dictionary of named metrics.\n",
        "\n",
        "üìä We use both accuracy (overall performance) and F1-score (accounts for class imbalance)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNIg2XtS4gmn"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Evaluation metric function used by Hugging Face's Trainer\n",
        "def compute_metrics(batch):\n",
        "    # Get predicted class (highest logit)\n",
        "    predictions = batch.predictions.argmax(-1) #Y_hat\n",
        "    # Actual class labels\n",
        "    labels = batch.label_ids                   #Y\n",
        "    accuracy = accuracy_score(labels, predictions)\n",
        "    f1 = f1_score(labels, predictions, average=\"weighted\")\n",
        "    return {\"accuracy\": accuracy, \"f1\": f1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpRgyFOWQ-Wv"
      },
      "source": [
        "### ü§ó What is Hugging Face‚Äôs `Trainer`?\n",
        "\n",
        "`Trainer` is a high-level class that abstracts away much of the boilerplate code for training and evaluation.\n",
        "\n",
        "It manages:\n",
        "- Training loop\n",
        "- Evaluation\n",
        "- Logging\n",
        "- Checkpointing\n",
        "- Pushing models to the Hub\n",
        "\n",
        "We‚Äôll also use `TrainingArguments` to configure its behavior (like batch size, learning rate, number of epochs, etc.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl0Rgq0_K5Yu"
      },
      "source": [
        "üîê To push our fine-tuned model to your Hugging Face account, we need to authenticate via the Hub.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FBVuViVK5Yv"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# Log in to your Hugging Face account to enable model uploads\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37zGSSaLK5Yv"
      },
      "source": [
        "### üèÅ Start Training the Model\n",
        "\n",
        "We‚Äôll configure our training using the `TrainingArguments` class.\n",
        "\n",
        "üß™ We‚Äôll also enable logging with tools like `wandb` (Weights & Biases), which is useful for:\n",
        "- Tracking model performance in real-time\n",
        "- Visualizing training loss and metrics\n",
        "- Debugging and optimizing\n",
        "\n",
        "You can set `output_dir` to determine where training artifacts (like checkpoints and logs) are stored.\n",
        "\n",
        "üî•üî•üî•Now let's launch the full fine-tuning of DistilBERT with logging and evaluation after each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_kpgobB2UAg"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "#emotionsDatasetEncoded\n",
        "batch_size = 64 #128, 256\n",
        "logging_steps = len(emotionsDatasetEncoded[\"train\"])//batch_size\n",
        "modelName = f\"{model_checkpoint}-finetuned-emotion\"\n",
        "\n",
        "# Configure training parameters\n",
        "trainingArgs = TrainingArguments(output_dir = modelName,\n",
        "                                 num_train_epochs = 2,\n",
        "                                 per_device_train_batch_size = batch_size,\n",
        "                                 per_device_eval_batch_size = batch_size,\n",
        "                                 learning_rate = 2e-5,\n",
        "                                 weight_decay = 0.01,\n",
        "                                 eval_strategy = \"epoch\",\n",
        "                                 disable_tqdm = False,\n",
        "                                 logging_steps = logging_steps,\n",
        "                                 push_to_hub = True,\n",
        "                                 log_level = \"error\"\n",
        "                                 )\n",
        "\n",
        "# Instantiate the Trainer\n",
        "trainer  = Trainer(model = tf_classifier,\n",
        "                   args = trainingArgs,\n",
        "                   compute_metrics = compute_metrics,\n",
        "                   train_dataset = emotionsDatasetEncoded[\"train\"] ,\n",
        "                   eval_dataset = emotionsDatasetEncoded[\"validation\"] ,\n",
        "                   tokenizer = tokenizer)\n",
        "\n",
        "# Start the fine-tuning process!\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìà Let‚Äôs compare our fine-tuned Transformer model to the earlier **Logistic Regression baseline**.\n",
        "\n",
        "You‚Äôll likely see a significant improvement thanks to end-to-end learning!\n",
        "\n",
        "\n",
        ">üí°üí°üí° Before we dive deeper in the future with fine tuning, we‚Äôll have a couple of short dedicated tutorials to explain **prompt engineering** and **template-based fine-tuning**. In these upcoming lessons, we will explain how prompting works with language models.\n",
        "\n",
        "‚û°Ô∏è Stay tuned for that lesson next!\n"
      ],
      "metadata": {
        "id": "L3uvIr5Ca0xM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjxUYch4woqc"
      },
      "outputs": [],
      "source": [
        "# Run evaluation on the validation set\n",
        "modelPredictions = trainer.predict(emotionsDatasetEncoded[\"validation\"])\n",
        "\n",
        "# Show evaluation results\n",
        "print(\"Model Predictions Validation Report\\n================================\\n\")\n",
        "print(modelPredictions.metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üß† The result also includes **raw logits** ‚Äî the model‚Äôs confidence scores for each class.\n",
        "\n",
        "We can use `np.argmax()` to convert those into predicted class indices, just like we did earlier with Logistic Regression.\n"
      ],
      "metadata": {
        "id": "Y4zuw3rBcKGd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSfDKh6VK5Y0"
      },
      "outputs": [],
      "source": [
        "# Check shape of prediction logits from validation set\n",
        "print(\"Model Predictions Shape: \",modelPredictions.predictions.shape)\n",
        "\n",
        "# First prediction: logits for all classes\n",
        "print(\"First Row predicitons: \", modelPredictions.predictions[0])\n",
        "\n",
        "# Convert logits to predicted label indices using argmax\n",
        "predictions_Labels = np.argmax(modelPredictions.predictions, axis=1)\n",
        "\n",
        "# Check prediction label shapes and first result\n",
        "print(\"Model Predictions Labels Shape: \",predictions_Labels.shape)\n",
        "print(\"First Row Label predicitons: \", predictions_Labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzFpIySqK5Y3"
      },
      "source": [
        "#### üîé Error Analysis\n",
        "\n",
        "Before wrapping up, let‚Äôs perform some basic **error analysis**.  \n",
        "\n",
        "\n",
        "Let's build a function that returns both the loss and the predicted label for each sample. By analyzing the highest-loss examples, we can uncover where the model struggles the most.\n",
        "\n",
        "üìå This final analysis gives a clear look at the most difficult examples for our model ‚Äî valuable for debugging or data augmentation.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9DgEYs-xenB"
      },
      "outputs": [],
      "source": [
        "from torch.nn.functional import cross_entropy\n",
        "\n",
        "# Function to compute per-example loss and predicted labels\n",
        "def get_model_output_loss(batch):\n",
        "  model_inputs = {k:v.to(device) for k,v in batch.items() if k in tokenizer.model_input_names}\n",
        "  with torch.no_grad():\n",
        "    model_outputs = tf_classifier(**model_inputs)\n",
        "    loss = cross_entropy(model_outputs.logits, batch[\"label\"].to(device), reduction=\"none\")\n",
        "  return {\"loss\": loss.cpu().numpy(), \"predictedLabel\": torch.argmax(model_outputs.logits, axis=-1).cpu().numpy()}\n",
        "\n",
        "# Reformat and apply to validation set\n",
        "emotionsDatasetEncoded.set_format(\"torch\", columns =[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "emotionsDatasetEncoded[\"validation\"] = emotionsDatasetEncoded[\"validation\"].map(get_model_output_loss, batched=True, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QteqQL8fK5Y4"
      },
      "source": [
        "üìâ This adds new columns to the validation set: loss and predictedLabel, giving us insight into model confidence.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert numeric label ID to string class name\n",
        "def label_int2str(row):\n",
        "    return emotionDataset[\"train\"].features[\"label\"].int2str(row)"
      ],
      "metadata": {
        "id": "dN7b35BjjOMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRdNS0L30OXf"
      },
      "outputs": [],
      "source": [
        "# Convert validation dataset to pandas\n",
        "emotionsDatasetEncoded.set_format(\"pandas\")\n",
        "\n",
        "#Last two columns added\n",
        "columns = [\"text\",\"label\", \"predictedLabel\", \"loss\"]\n",
        "\n",
        "# Create DataFrame\n",
        "validationDF = emotionsDatasetEncoded[\"validation\"][:][columns]\n",
        "\n",
        "# Convert label integers to strings\n",
        "validationDF[\"label\"]= validationDF[\"label\"].apply(label_int2str)\n",
        "validationDF[\"predictedLabel\"] = (validationDF[\"predictedLabel\"].apply(label_int2str))\n",
        "\n",
        "# Display the top 10 misclassified examples with highest loss\n",
        "print(\"üîù Top 10 Misclassified Texts\\n\")\n",
        "validationDF.sort_values(\"loss\", ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH3K-npUK5Y6"
      },
      "source": [
        "#### üíæ Saving and Sharing the Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aiZPmKkK5Y6"
      },
      "source": [
        "ü§ó Hugging Face makes it easy to share your model with the community ‚Äî just like we downloaded DistilBERT from the Hub, you can push your fine-tuned model back to it.\n",
        "\n",
        "Let‚Äôs publish our trained model with one command.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHCPvzYl1l34"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Push model to your Hugging Face account with a commit message\n",
        "trainer.push_to_hub(commit_message=\"DistilBert trained with Emotion dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaCWpzES4Icx"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the model from the Hugging Face Hub\n",
        "model_checkpoint = \"moghalis/distilbert-base-uncased-finetuned-emotion\"\n",
        "classifier = pipeline(\"text-classification\", model=model_checkpoint)\n",
        "\n",
        "# Try a test example\n",
        "tweet = \"I am really happy. I know how to do fine tuning with hugging face!\"\n",
        "prediction = classifier(tweet, return_all_scores=True)\n",
        "prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üåç We successfully deployed our custom model ‚Äî now anyone can use it just like this."
      ],
      "metadata": {
        "id": "QjS93xOud-op"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvEkJu2NUT8c"
      },
      "source": [
        "## ‚úÖ Conclusion\n",
        "\n",
        "In this tutorial, we explored two approaches to fine-tuning a Transformer:\n",
        "\n",
        "1. **Feature extraction** using `AutoModel` and traditional ML\n",
        "2. **End-to-end fine-tuning** using `AutoModelForSequenceClassification` and `Trainer`\n",
        "\n",
        "We wrapped up with evaluation, error analysis, and even model sharing!\n",
        "\n",
        "üöÄ Keep experimenting with different architectures and datasets ‚Äî and don‚Äôt forget to share your models on the Hub!\n",
        "\n",
        "üëâ Like this tutorial? Subscribe, give it a ‚≠ê on GitHub, and follow for more hands-on NLP content!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXKoJugDdAXb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}